{"meta":{"title":"Ruby's Blogs","subtitle":null,"description":null,"author":"RubyFXD","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"Docker 基本概念","slug":"Docker-基本概念","date":"2020-01-02T15:11:39.000Z","updated":"2020-01-02T15:27:50.535Z","comments":false,"path":"2020/01/02/Docker-基本概念/","link":"","permalink":"http://yoursite.com/2020/01/02/Docker-基本概念/","excerpt":"","text":"一、Why Docker?&emsp;&emsp;Problem - 软件开发中的问题:&emsp;&emsp;环境配置是软件开发过程中的难题，不同的计算机可能使用不同的操作系统，有不同的系统配置。一个软件可以执行的必要条件是，软件的运行依赖存在，并且对操作系统进行了正确的配置。&emsp;&emsp;所以当软件从开发环境移植到，客户机运行环境、或者测试环境的时候，可能因为操作系统设置不正确或者缺少依赖，而不能运行。比如运行JAVA应用程序，必须有JRE，还需要配置JAVA环境变量。&emsp;&emsp;简而言之，软件运行因为需要在不同机器上配置环境变量，引入依赖库。就会变的十分复杂，操作冗余。 &emsp;&emsp;Solution - 软件带所需环境安装：&emsp;&emsp;（1）虚拟机&emsp;&emsp;虚拟机是一种软件带环境安装的方法，虚拟机是对硬件的一种抽象，可以将一个服务器拆分成多台服务器。Hypervisor允许多个VM在一个物理机上运行，每个VM都包括一份独立的对操作系统，应用软件，必要的二进制文件和库的复制。从而做到软件带环境安装到虚拟机。 图1 VM &emsp;&emsp;（2）Docker&emsp;&emsp;Docker是软件带环境安装的另外一种方法，docker容器是对app层的抽象，Docker容器将代码和依赖打包。多个容器可以同时运行在一个物理机上 并且共享操作系统内核，每一个容器都可以被视为相互独立且隔离的进程。容器是基于沙盒机制的，容器之间不会有任何接口。 图2 Docker &emsp;&emsp;（3）Docker容器 vs VM 容器是进程级别的，只包含用到的资源，容量小，启动速度快。容器运行在操作系统上，和宿主机器共享硬件资源和操作系统，可以实现资源的动态分配。容器包含应用和依赖包，与其他容器共享内核。在宿主操作系统中，用户控件以分离的进程运行。可以最优化操作系统内存的应用。 虚拟机需要模拟整个机器包括硬件，每个虚拟机都有自己的操作系统。虚拟机被启动之后，分配给虚拟机的所有资源都会被占用，虚拟机除了包含应用程序，二进制和库，还有一个完成的用户操作系统。所以，资源有些浪费，并且启动速度缓慢。 二、什么是Docker?&emsp;&emsp;Docker是一个开源的容器引擎，开发者可以将应用程序和程序所依赖的包，一起打包到一个文件中，运行文件即可产生一个虚拟容器，程序在虚拟容器中运行，无需考虑物理机运行环境的不同。 三、Docker架构及基本概念 图3 Docker架构 Docker架构图中包含如下几个部分：&emsp;&emsp;（1）Docker Client客户端：Docker客户端可以通过命令行或者其他工具，调用Docker Engin API与 Docker守护进程Daemon进行通信。&emsp;&emsp;（2）Docker Machine：是一个简化了的docker命令行安装工具，通过简单的命令可以在不同的平台上安装docker。&emsp;&emsp;（3）Docker Hosts主机：运行Docker守护进程daemon和容器container的，物理或者虚拟机器，可以是local本地主机或者remote远程主机。&emsp;&emsp;（4）Docker Daemon守护进程：运行在宿主机后台，等待接收来自客户端的消息，Docker客户端通过调用API与守护进程交互，执行对应操作，如容器的创建，运行，暂停，终止，删除。&emsp;&emsp;（5）Docker Image镜像：镜像image是用来创建容器container的模版，一个宿主机上可以存在多个镜像（image1、image3), 一个镜像可以创建多个容器（container3a、container3b），类似于Java中类与对象的关系。镜像可以从Docker Hub上下载。&emsp;&emsp;（6）Docker Container容器：容器是通过镜像创建的，容器是动态的，镜像是静态的，一个容器可以运行一个或者一组应用，内部可以安装任何软件和库文件，可以对内部环境变量做任何配置。每个容器之间都是相互隔离的，容器之间可以共享操作系统内核，但是不共享容器内部资源。&emsp;&emsp;（7）Docker Registries注册服务器：注册服务器是存放镜像仓库的地方，一个registry可以包含多个repository, 每个repository中可以包含多个镜像，如不同版本的相关iamges, ubuntu:2.2, ubuntu:2.3 ….. ubuntu:latest。&emsp;&emsp;（8）Docker Repository仓库：用来保存镜像，可以理解为代码控制中的代码仓库，类似于Maven的中央仓库。Docker公司的官方仓库存放在Docker Hub。Repository是开源项目，所以任何人都可以部署自己的私有仓库并注册在private registry，类似push到github。 Docker架构：&emsp;&emsp;Docker采用C/S架构，客户端通过调用远程API管理Docker容器，Docker daemon作为服务端接受来自客户端的请求，并执行相应任务。客户端可服务器可以在同一个宿主机，可以个在不同机器上。 四、容器云技术 kubernets和swarm 两者都是容器的编排服务，把底层的宿主主机抽象化，然后将应用从镜像开始，以docker的方式部署到宿主机上 Kubernetes: 轻量级、以实现核心功能为重、实施快速、适合小规模部署 Swarm: 企业级、功能全、支持场景多、适合做企业级docker云方案","categories":[{"name":"后端","slug":"后端","permalink":"http://yoursite.com/categories/后端/"},{"name":"Docker","slug":"后端/Docker","permalink":"http://yoursite.com/categories/后端/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}],"keywords":[{"name":"后端","slug":"后端","permalink":"http://yoursite.com/categories/后端/"},{"name":"Docker","slug":"后端/Docker","permalink":"http://yoursite.com/categories/后端/Docker/"}]},{"title":"五大常用算法 - 贪心法","slug":"五大常用算法-贪心法","date":"2019-07-16T12:25:57.000Z","updated":"2019-07-21T07:25:02.656Z","comments":false,"path":"2019/07/16/五大常用算法-贪心法/","link":"","permalink":"http://yoursite.com/2019/07/16/五大常用算法-贪心法/","excerpt":"","text":"贪心法1. 贪心法概念2. 贪心法步骤3. 贪心法使用条件4. 常见问题","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"五大常用算法","slug":"算法/五大常用算法","permalink":"http://yoursite.com/categories/算法/五大常用算法/"},{"name":"贪心法","slug":"算法/五大常用算法/贪心法","permalink":"http://yoursite.com/categories/算法/五大常用算法/贪心法/"}],"tags":[{"name":"五大常用算法","slug":"五大常用算法","permalink":"http://yoursite.com/tags/五大常用算法/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"五大常用算法","slug":"算法/五大常用算法","permalink":"http://yoursite.com/categories/算法/五大常用算法/"},{"name":"贪心法","slug":"算法/五大常用算法/贪心法","permalink":"http://yoursite.com/categories/算法/五大常用算法/贪心法/"}]},{"title":"五大常用算法 - 动态规划","slug":"五大常用算法-动态规划","date":"2019-07-13T14:03:56.000Z","updated":"2019-07-14T08:00:43.710Z","comments":false,"path":"2019/07/13/五大常用算法-动态规划/","link":"","permalink":"http://yoursite.com/2019/07/13/五大常用算法-动态规划/","excerpt":"","text":"动态规划1. 动态规划概念动态规划与分治法形似，都是通过组合子问题的解来去接原问题。分治法，将问题划分为互不相交的子问题，递归地求解子问题，再将它们的解组合起来，求出原问题的解。动态规划，应用于子问题重叠的情况，即不同子问题具有公共的子子问题。在子问题重叠的情况下，分治法会反复求解那些公共子子问题，而动态规划对每个子问题只求解一次，将解保存在一个表格中，从而无需反复求解公共子子问题。所以动态规划问题中，子问题间往往不是独立的，即下一子阶段的解是建立在上一子阶段的解的基础上，进一步求解的，反复递推得到最终解；分治法问题中，子问题间相互独立，子问题之间的解相互独立，最终解由个子阶段的解合并而成。 动态规划常用来求解最优化问题。 2. 动态规划步骤动态规划，是一个包含多个子阶段的状态的状态链。从初始状态开始，经过中间决策序列中的状态转移，达到最终状态。决策过程如下图所示： (1) 动态规划问题求解步骤 划分问题阶段：根据时间或者空间顺序将问题划分成有序的问题阶段，并且确定初始阶段 抽象出状态变量：找到各子阶段状态的共同特征，得到状态变量的抽象表达 明确决策和状态转移：决策是状态转移的触发点，决策用来过滤本阶段的可行解或者最优解，从而进行状态转移。 终止条件：动态规划是一个决策链，终止条件定义了递推过程的终止点。 (2) 算法通用实现 动态规划算法有两种实现方式，递归和迭代。用递归实现，因为公共子问题的解是保存在表格中的，所以即便使用了递归，也不会出现重复公共子问题计算。 1234567891011dp[row-1][col-1] 二维数组存储状态变量dp[0][0]=XXX // 确定初始状态for(int i=1; i&lt;row; i++) dp[i][0]=dp[i-1][0]+option // 决策+状态转移方程----&gt;递推首列for(int j=1; j&lt;coll j++) dp[0][j]=dp[0][j-1]+option // 决策+状态转移方程----&gt;递推首行for(int i=1; i&lt;row; i++) for(int j=1; j&lt;col; j++) dp[i][j]=dp[i][j-1]/dp[i-1][j]+option // 决策+状态转移方程----&gt;递推中间状态 return dp[row-1][col-1](最终状态)/或者状态转移过程 3. 动态规划适用条件分治法应用的问题具有如下性质： (1) 最优子结构性质，即问题的最优解，包含了其中子结构的最优解。 (2) 重叠子问题性质，下一子阶段的解是建立在上一阶段子问题解的基础上，递推得出的。则之前子子问题是反复包含于后续子问题中的。 (3) 后无效性质，某状态的转移，只与当前状态有关，与其后决策的影响。 4. 常见问题《LeetCode》相关问题 判断字符串C是否是字符串A和B的交叉拼接结果 矩阵中左上到右下的最短路径 矩阵有障碍点，左上到右下的可行路径数目","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"五大常用算法","slug":"算法/五大常用算法","permalink":"http://yoursite.com/categories/算法/五大常用算法/"},{"name":"动态规划","slug":"算法/五大常用算法/动态规划","permalink":"http://yoursite.com/categories/算法/五大常用算法/动态规划/"}],"tags":[{"name":"五大常用算法","slug":"五大常用算法","permalink":"http://yoursite.com/tags/五大常用算法/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"五大常用算法","slug":"算法/五大常用算法","permalink":"http://yoursite.com/categories/算法/五大常用算法/"},{"name":"动态规划","slug":"算法/五大常用算法/动态规划","permalink":"http://yoursite.com/categories/算法/五大常用算法/动态规划/"}]},{"title":"递归和迭代的区别","slug":"递归和迭代的区别","date":"2019-07-13T12:19:06.000Z","updated":"2019-07-13T13:03:59.519Z","comments":false,"path":"2019/07/13/递归和迭代的区别/","link":"","permalink":"http://yoursite.com/2019/07/13/递归和迭代的区别/","excerpt":"","text":"相同点递归和迭代都是循环 不同点(1)循环方式不同递归是通过调用自身函数进行循环；迭代是通过函数内的一段代码段进行循环。迭代的代码段中的本地循环的结果变量，将作为下次循环的输入变量结算下次循环的结果。 (2)循环结束条件不同递归，当满足终止条件（可以直接解决问题的阀值）逐层返回子问题结果结束；迭代使用计数器计数器，如for(int i; i条件; i操作)来控制循环结束。 (3)效率不同循环次数逐渐变大时，迭代的效率会高于递归的效率，因为递归调用自身的函数需要不断入栈出栈，所以消耗的时间比较长。并且迭代像是自底向上的计算，每一次循环的结果都作为下一次循环的输入；迭代更像是自顶向下的求解，会存在一些冗余计算。 e.g. 斐波那契数组 迭代和递归实现","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"知识点","slug":"算法/知识点","permalink":"http://yoursite.com/categories/算法/知识点/"}],"tags":[{"name":"算法知识点","slug":"算法知识点","permalink":"http://yoursite.com/tags/算法知识点/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"知识点","slug":"算法/知识点","permalink":"http://yoursite.com/categories/算法/知识点/"}]},{"title":"五大常用算法 - 分治法","slug":"五大常用算法-分治法","date":"2019-07-13T06:10:52.000Z","updated":"2019-07-13T12:17:16.981Z","comments":false,"path":"2019/07/13/五大常用算法-分治法/","link":"","permalink":"http://yoursite.com/2019/07/13/五大常用算法-分治法/","excerpt":"","text":"分治法1. 分治概念分治，即分而治之。将比较复杂的问题分解成多个相同、或者相似的子问题，再将子问题分解成更多的子问题，直到问题的规模可以简单的直接求解。将子问题的解合并即为原问题的解。 例如，对于一个问题规模为n的原问题，如果该问题可以很容易求解，则直接求解。否则可以拆分为k(1&lt;k&lt;=n)个规模较小的子问题，这些子问题相互独立并且与原问题相同或相似。通过递归，反复使用分治法，可以将子问题分解为更小规模的K’(1&lt;k’&lt;=k)个子问题，直到可以简单求解。合并子问题解得到原问题解。 tips: 分治和递归通常是相辅相成，同时使用的。tips: 分治法的算法复杂性随着问题规模n的增大而增大。tips: 当递归消耗大的时候，可以考虑使用迭代。递归更像是自顶向下解决问题，迭代是自底向上解决问题。迭代的效率比递归高，不需要重复计算。 2. 分治法步骤(1) 分治法中，每一层递归都包含三个步骤 分解(Divide): 将问题划分为一些子问题，子问题的形式与原问题一样，只是规模更小。 解决(Conquer): 递归地求解子问题。如果子问题的规模足够小，则停止递归，直接求解。 合并(Combine): 将子问题的解组合成原问题的解 (2) 算法通用实现 123456789Divide-and-Conquer(P)if (P&lt;n0) // P是为题的规模，n0是问题可直接解出的阀值 return ADHOC(P) // ADHOC是分治法中的基本子算法。当P,n0时，可直接用ADHOC(P)求解else 将P分解成较小子问题P1,P2...Pk for i: from 1 to k solution i = Divide-and-Conquer(Pi) //递归解决每个子问题 Solution = Merge(solution 1..i) //合并子问题的解 return Solution tips: 找阀值n0+ADHOC()—–&gt;递归因子+递归函数 3. 分治法适用条件分治法应用的条件： (1) 问题的规模缩小到一定程度，就能简单的直接求解 (2)问题可以拆分为规模较小的相同子问题（最优子结构性） (3)子问题的解可以通过合并，得到原问题的解 (4)原问题分解得到的子问题是相互独立的，子问题之间没有重叠（相同子子问题） 如果(3)不能满足，则可以考虑使用贪心算法（局部最优解-&gt;整体最优解)，或者使用动态规划。 如果(4)不能满足，则可以使用动态规划，分治依然可以使用，但是需要重复递归，算法消耗大效率低。 4. 常见问题《剑指 offer》相关问题 斐波那契数列 跳台阶 变态跳台阶 矩形覆盖","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"五大常用算法","slug":"算法/五大常用算法","permalink":"http://yoursite.com/categories/算法/五大常用算法/"},{"name":"分治法","slug":"算法/五大常用算法/分治法","permalink":"http://yoursite.com/categories/算法/五大常用算法/分治法/"}],"tags":[{"name":"五大常用算法","slug":"五大常用算法","permalink":"http://yoursite.com/tags/五大常用算法/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"五大常用算法","slug":"算法/五大常用算法","permalink":"http://yoursite.com/categories/算法/五大常用算法/"},{"name":"分治法","slug":"算法/五大常用算法/分治法","permalink":"http://yoursite.com/categories/算法/五大常用算法/分治法/"}]},{"title":"String StringBuffer StringBuilder 区别和相互转化","slug":"String-StringBuffer-StringBuilder-区别和相互转化","date":"2019-07-12T10:10:46.000Z","updated":"2019-07-12T08:40:57.440Z","comments":false,"path":"2019/07/12/String-StringBuffer-StringBuilder-区别和相互转化/","link":"","permalink":"http://yoursite.com/2019/07/12/String-StringBuffer-StringBuilder-区别和相互转化/","excerpt":"","text":"1.区别 String是字符串常量，由String创建的字符内容是不可改变的。String内部是由char数组实现的，这个char数组有final修饰符。所以String创建的字符串对象内容是不可变的。 StringBuffer，StringBuilder 创建的是字符串变量，创建的字符串是可以改变的。StringBuffer, StringBuilder 都继承自AbstractStringBuilder, 内部也是用char数组实现的，但是char没有final修饰。所以创建的字符是可变的。 StringBuffer是线程安全的，append函数前有synchronized关键字修饰；StringBuilder不是线程安全的，所以更加高效，支持StringBuffer的所有操作。因此，一般单线程情况下优先使用StringBuilder。 2.相互转换 String —–&gt; StringBuffer或者StringBuilder 1234567891011String s=&quot;test&quot;;//通过构造函数StringBuffer sBuffer= new StringBuffer(s);StringBuilder sBuilder= new StringBuilder(s);//通过append方法StringBuffer sBuffer= new StringBuffer();sb.append(s);StringBuilder sBuilder= new StringBuilder();sbr.append(s); StringBuffer或StringBuilder —–&gt; String 1234567StringBuffer sBuffer= new StringBuffer(&quot;bufferTest&quot;);//通过构造函数String s1= new String(sBuffer);//通过toString方法String s2= new sBuffer.toString(); 3. String常用方法==比较引用，equals比较内容 (1) 构造方法 123456789101112131415161718//无参构造 publc String()String s1= new String();//public String(String value);String s2= new String(&quot;abc&quot;);String s3= new String(s2);//字符数组创建 public String(char[] value)char[] cArray= &#123;&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;&#125;;String s4= new String(cArray);//字符数组指定字符创建 public String(char[] value, int startIndex, int numChars)char[] cArray= &#123;&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;&#125;;String s5= new String(cArray, 1, 3); //相当于s5= new String(&quot;bcd&quot;)//byte数组创建 public String(byte[] value)byte[] bArray= &#123;97,98,65,66&#125;String s6= new String(bArray); //相当于s6=new String(&quot;abAB&quot;) (2)字符串长度 123String s7 = new String(&quot;123456&quot;);int s7Length= s7.length(); // s7Length=6##### 3. StringBuffer常用方法 (3)字符串比较 12345678910//public int compareTo(String s) 判断和s的大小，比参数大返回正整数，小返回负整数，相等返回0//public int compareToIgnoreCase(String s) 同上，忽略大小写//public boolean equals(String s) 对比字符串内容是否一致//public boolean qualsIgnoreCase(String s) 同上，忽略大小写String str=&quot;ABCabc&quot;str.compareTo(&quot;BA&quot;); //-1str.compareToIgnoreCase(&quot;abcabc&quot;); //0str.equals(&quot;abcabc&quot;); //falsestr.equalsIgnoreCase(&quot;abcabc&quot;); //true (4)字符串连接 12//public String concat(String s)String s8= &quot;abc&quot;.concat(&quot;def&quot;).concat(&quot;ghi&quot;); //相当于s8=&quot;abs&quot;+&quot;def&quot;+&quot;ghi&quot; (5)字符串分割 123456//String[] split(String s)String s9=&quot;string split test with space&quot;;String[] sArray1= s9.split(&quot;\\\\s&quot;);String s10=&quot;string#split!test with|space&quot;;String[] sArray2= s10.split(&quot;#!\\\\s|&quot;); (6)字符串截取子串 1234567891011121314//String trim() 截取字符串两端的空格，中间出现的空格不做处理String s11= &quot; a b c d &quot;;String s12=s11.trim(); // &quot;a b c d&quot;//public char charAt(int index) 获取字符串某一位置的字符String s13=&quot;abcdefg&quot;;char c=s13.charAt(1); // &apos;b&apos;//public String substring(int beginIndex) 从beginIndex开始到结束的子串返回为新字符串//public String substring(int beginIndex, int endIndex)//从beginIndex开始，到endIndex-1结束的，返回子串为新的字符串String s13=&quot;abcdefg&quot;;String s14=s13.substring(2); // &quot;cdefg&quot;String s15=s13.substring(1,5); // &quot;bcde&quot; (7)字符替换及大小写转换 1234567891011121314//public String replace(char/String old, char/String new) 用新的字符或者字符串替换所有旧的字符或者字符串，并返回新的字符串//public String replaceFirst(String regex, String replacement) 用replacement替换字符串中第一个匹配到regex的子串，并返回新的字符串//public String replaceAll(String regex, String replacement) 用replacement替换字符串中的所有匹配到regex的子串，并返回新的字符串String s16=&quot;ABCabcABC&quot;;String s17=s16.replace(&apos;A&apos;,&apos;a&apos;); // &quot;aBCabcaBC&quot;String s18=s16.replace(&quot;ABC&quot;,&quot;abc&quot;); // &quot;abcabcabc&quot;String s19=s16.replaceFirst(&quot;ABC&quot;,&quot;abc&quot;); // &quot;abcabcABC&quot;String s20=s16.replaceAll(&quot;ABC&quot;,&quot;abc&quot;); // &quot;abcabcabc&quot;//public String toLowerCase() 字符串中所有字母转化为小写，并返回新字符串//public String toUpperCase() 字符串中所有字母转化为大写，并返回新字符串String s21=&quot;abcABC&quot;String s22=s21.toLowerCase(); // &quot;abcabc&quot;String s23=s21.toUpperCase(); // &quot;ABCABC&quot; (8)字符，子串查找 1234567891011121314151617String s24=&quot;abcdefg&quot;//boolean contains(String s) 判断字符串中是否包含子串ss24.contains(&quot;bcd&quot;); // true//public int indexOf(int ch/String s) 查找左起第一个匹配到的字符或者子串，返回首次首次出现的位置，没有返回-1//public int indexOf(int ch/String s, int fromIndex) 从fromIndex开始包括fromIndex位的左起首次匹配位置，没有返回-1//public int lastIndexOf(int ch/String s) 同上，右起查找//public int lastIndexOf(int ch/String s, int fromIndex) 同上，右起查找int i1= s24.indexOf(&apos;c&apos;); // 2int i2= s24.indexOf(67); // 2int i3= s24.indexOf(&quot;abc&quot;); // 0//boolean startWith(String prefix) 判断是否以prefix开始//boolean endWith(String suffix) 判断是否以 suffix结束s24.startWith(&quot;abc&quot;); // trues24.endWith(&quot;kkk&quot;); // false (9)与基本类型、byte[]数组、char[]数组的转化 123456XXX xxx= parseXXX(String s)String s= String.valueOf()String s= new String(byte[])String s= new String(char[])char[] chars= s.toCharArray();byte[] bytes= s.getBytes(); 4.StringBuffer常用方法1StringBuffer buffer= StringBuffer(); (1) 添加删除查找 1234567//append(int/char/float/double/long/boolean/char[]/String) 追加buffer.append(&quot;stringbuffer&quot;);//insert(int offset, int/char/float/double/long/boolean/char[]/String) 指定offset位置插入buffer.insert(0,&quot;123&quot;); // &quot;123stringbuffer&quot;buffer.deleteCharAt(0); // &quot;23stringbuffer&quot;buffer.delete(0,1); // &quot;stringbuffer&quot;buffer.indexOf(&quot;buffer&quot;); //6 (2) 替换修改 1234//replace(int start, int end, String str)buffer.replac(0,2,&quot;rts&quot;); // &quot;rtsingbuffer&quot;//setCharAt(int index, char c)buffer.setCharAt(0,&apos;0&apos;); // &quot;0tsingbuffer&quot; (3) 反转 1buffer.reverse(); //&quot;reffubgnist0&quot;","categories":[{"name":"后端","slug":"后端","permalink":"http://yoursite.com/categories/后端/"},{"name":"Java","slug":"后端/Java","permalink":"http://yoursite.com/categories/后端/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[{"name":"后端","slug":"后端","permalink":"http://yoursite.com/categories/后端/"},{"name":"Java","slug":"后端/Java","permalink":"http://yoursite.com/categories/后端/Java/"}]},{"title":"Hexo 图片无法显示问题","slug":"Hexo-图片无法显示问题","date":"2019-07-09T06:51:29.000Z","updated":"2019-07-09T06:59:06.369Z","comments":false,"path":"2019/07/09/Hexo-图片无法显示问题/","link":"","permalink":"http://yoursite.com/2019/07/09/Hexo-图片无法显示问题/","excerpt":"","text":"图片不显示问题Hexo的source文件夹下，除了存放博客的md文件，还可以存放博客需要的图片，并通过Markdown引用图片的语法，引用图片到文章。 1![图片名称](图片相对路径) 在启动本地服务器后，本地测试图片显示没有问题。当把文章部署到git之后，远程访问图片会出现不显示的问题。因为图片的相对路径在source文件夹下和public文件夹下的相对路径不一致，public文件夹下会在图片的相对地址前加上年月日文件结构，因此访问不到。 一般有两种解决方法： 资源文件夹 + 相对地址引用的图片标签插件 图床 + 绝对地址 图片标签插件图片存在本地source文件夹下和md文档同名的资源文件夹下，通过使用图片标签插件，使用相对路径引用图片。 1.开启资源文件夹功能打开hexo文件夹根目录下的 _config.yml配置文件，将post_asset_folder设置为true。 123_config.ymlpost_asset_folder: true 在新建文章时，_post文件夹下除了md文档外，还会出现同名文件夹用来存放图片，如下图。 Figure 1 资源文件夹 2.下载图片标签插件安装hexo-asset-image插件，用来自动转换图片路径。 12$ cd hexo博客目录$ npm install https://github.com/CodeFalling/hexo-asset-image --save 3.使用插件引用图片将需要md文章中需要饮用的文件放在同名资源文件夹中，通过标签插件直接引用。格式为{ %asset_img 图片名称.图片格式 [title] %}。标签插件会将自动转化图片的相对路径和public种的日期文件结构一致，解决路径不一致问题。 图床图床是将要引用的图片放在可托管的服务器上，在文章中引用服务器上图片的绝对地址，同样不会造成图片文件路径不一致的问题。 常用图床： 1.七牛云2.SM.MS3.ImgURL4.postimg5.聚合图床","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"}],"keywords":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"}]},{"title":"OSX下使用Hexo和GitHub搭建个人博客","slug":"OSX下使用Hexo和GitHub搭建个人博客","date":"2019-07-06T04:38:05.000Z","updated":"2019-07-09T02:33:15.228Z","comments":false,"path":"2019/07/06/OSX下使用Hexo和GitHub搭建个人博客/","link":"","permalink":"http://yoursite.com/2019/07/06/OSX下使用Hexo和GitHub搭建个人博客/","excerpt":"","text":"一、准备1. 什么是Hexo?Hexo是一个使用node.js开发的命令行脚本工具。Hexo是一个简单、高效的博客框架，可以把Markdown文件翻译成html页面。将Hexo在本地生成的静态网站部署到web服务器上，形成可供访问的个人博客站点。 2. GitHub账号Hexo是一个静态博客框架，产生的html文件及相关文件需要部署到web服务器上。 GitHub Pages是面向用户、组织和项目的公共静态页面搭建托管服务，站点可以被免费托管在GitHub上。GitHub Pages相当于一个web服务器，可以通过git将本地Hexo生成的静态站点上传到GitHub Pages上。 3. node.jsnode.js是一种javascript的运行环境，它把浏览器的解释器封装起来，支持javascript脱离浏览器运行。Hexo是基于node.js开发的工具，需要node.js支持，所以安装Hexo之前确认安装node.js. 4. 环境配置 git+node.js(1) 检查是否安装git和node.js 12$ git --version$ node -v 如果已经安装，跳到二、Hexo安装配置；如果没有安装mac用户可以直接使用Homebrew命令安装。 (2)Homebrew安装 /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 检查Homebrew是否安装成功: $ brew -v (3) git安装： 传送门：git安装及配置 $ brew install git (4) node安装： 传送门： 使用Homebrew安装node.js $ brew install node 查看安装版本 12$ node --version$ npm --version tips: npm是和node.js一起安装的包管理工具，用来管理nodejs的第三方插件，支持用户从npm服务器下载第三方包到本地使用。 二、Hexo安装配置传送门：Hexo官方使用文档 1.使用npm进行Hexo安装$ npm install -g hexo-cli tips: 可能会出现permission denied error，如图1；解决方法如图二。 Figure 1 Hexo Install Error Figure 2 Solution 2. Hexo建站Hexo安装完成后，执行以下命令，Hexo将会在指定文件夹中新建所需要的文件。 1234$ hexo init &lt;folder&gt;e.g. $ hexo init mybolgs$ cd &lt;folder&gt;$ npm install 命令执行后，指定文件夹下的目录结构如下： Figure 3 目录结构 _config.yml是网站的配置文件，包含网站的配置信息 package.json应用程序信息 Scaffolds模版文件夹，新建文章的时候，Hexo会根据scaffold中的模版建立文件。Hexo模版是在新建markdown文件中默认的填充内容 source资源文件夹，存放用户资源。markdown和HTML文件会被解析并放到public文件夹，其他文件（除了被忽略文件）将会被拷贝到puclic文件夹。 Themes主题文件夹，Hexo会根据主题来生成相应的静态页面 3.Hexo站点测试$ hexo server hexo server 命令启动本地Hexo服务器，浏览器中输入: http://localhost:4000/ 如果访问到Hexo Hello World页面，证明 Hexo本地配置成功，个人站点建立成功。 4.Hexo _config.yml网站配置Hexo建站后，可以对_config.yml文件进行修改进行网站配置。修改_config.yml文件可以改变如网站标题，站点目录结构，文章分类标签，时区和日期格式等参数。 如果不进行设置，Hexo将采用_config.yml中默认值。新手可以跳过此步骤，先熟悉如何利用Hexo新建文章，生成html页面及部署等必要步骤。 传送门：Hexo官方_config.yml配置说明 5.Hexo 写博文（1）新建文章 $ hexo new [layout] &lt;title&gt; 该命令用来新建一篇文章，[layout]可以省略，如果没有设置，默认使用 _config.yml中default_layout 参数。layout用来指定生成的页面布局，默认default_layout是post（文章详情页面布局），即生成博文页面的html页面布局。 tips: 如果title标题中有空格，需要双引号扩起来。 e.g. $ hexo new &quot;The First Post Test&quot; 新建文章之后，若是默认的default_layout，则在source/ _posts文件夹下可以看到对应的md文件。 （2）生成Html页面 $ hexo generate 生成静态文件，将post下md文件翻译成对应的html页面（出现在public文件夹下）。 （3）启动本地服务器测试 $ hexo server 访问http://localhost:4000/，可以查看本地生成的个人站点，找到刚生成的文章。 三、Hexo部署到Git1.配置github pages在GitHub上新建一个仓库，命名为账户名.github.io。e.g. zhangsan.github.io github pages使用传送门 2.修改Hexo配置文件关联github pages（1）进入hexo站点文件夹下 12$ cd &lt;folder&gt;e.g. $ cd myblogs (2) 更改站点文件夹下_config.yml文件 1$ vi _config.yml 键入i，修改配置如下图。注意type, repository, branch冒号后面要加上一个空格，否则会报错。修改完成后，按esc,键入:wq保存退出。 Figure 4 _config.yml配置 3.安装插件1$ npm install hexo-deployer-git --save 4.将本地站点部署到GitHub上123456// 清理已经生成的弃用文件$ hexo clean// 生成新的站点$ hexo generate// 将生成的站点推送到GitHub Pages仓库$ hexo deploy 部署完成后，通过浏览器输入 https://账户名.github.io/ 访问你的个人博客。 四、更换主题1. 将主题clone到themes目录下可以在Hexo官方主题库中找到自己喜欢的主题，将主题对应的git仓库clone到本地博客文件夹themes目录下，使用 $ git clone url命令，例如下 12$ cd &lt;博客存放的folder&gt;$ git clone https://github.com/shenliyang/hexo-theme-snippet.git themes/hexo-theme-snippet themes/hexo-theme-snippet指定要将该主题clone到themes的hexo-theme-snippet文件下，效果如下图 Figure 5 clone主题 2. 修改配置文件 Figure 6 修改配置文件 圈1: Hexo根目录下的配置文件 _config.yml是用来配置整个站点的。打开此配置文件，修改主题。 theme: hexo-theme-snippet 圈2: Hexo/themes/某个主题文件夹 下的 _config.yml是用来配置这个主题的相关参数的，根据你clone的主题进行具体配置，作者大大写的更加详细～ snippet主题配置传送门。 3. 清理并重新生成站点更换主题之后，可以将之前主题生成的静态文件清理，然后根据当前主题重新生成站点，并部署。 123$ hexo clean$ hexo generate$ hexo deploy","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}],"keywords":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"}]},{"title":"Fantastic Fireworks Data Warehousing ETL Process Design","slug":"Fantastic-Fireworks-Data-Warehousing-ETL-Process-Design","date":"2019-03-25T18:50:13.000Z","updated":"2019-06-10T03:40:30.892Z","comments":false,"path":"2019/03/26/Fantastic-Fireworks-Data-Warehousing-ETL-Process-Design/","link":"","permalink":"http://yoursite.com/2019/03/26/Fantastic-Fireworks-Data-Warehousing-ETL-Process-Design/","excerpt":"","text":"1 Executive SummaryIn the era of the information explosion, the data warehouse can better assist the enterprises to manage and analyze their business data, so as to support decision-making. After modeling the star schema for the Fantastic Firework data warehouse, the next step is to pop the source data which may be stored in several independent systems into the designed data warehouse. The data integration and data quality are the critical issues in this step. This report will refine the previous star schema. Additionally, the report will discuss the design of the ETL process which is used to extract the raw data from the operational systems, transform the raw data to the unified data format to ensure the data quality, and load the processed data to the data warehouse. The data dictionary will be attached to provide the user with the definition of the schema and data content of the data warehouse, also it will navigate the end user where the integrated data is sourced from. Issues in ETL process design: The difference of date type between source systems, need to be unified in the data warehouse. Handle low data quality records and invalid inputs (like replacing letter “o” with number “0” in Customer table). Data integration across systems (like retrieving the cost per item from Inventory System to sales table). 2 Design of the ETL Process2.1 Customer Dimension Table Figure 1 Customer ETL Process Extraction: extract data from four source customer datasets. The mapping between source data and extracted data is shown in Figure 2. There are some data issues when extracting data from multiple data sources, including different data type with the same field; misspelled postcode(number 0 and letter O, incorrect length), detect the same customer in different stores. The extraction is executed weekly, the version field is used to handle the historical customer records. Figure 2 Customer source-target Data Mapping Transformation: The first step is to unify the field name and data type after reading from the four source files. Then replace the letter O to number 0 in postcode and handle the misspelled postcode with incorrect postcode length. Meanwhile, detect and merge the same customer record between Darwin and Melbourne store. After that, merge and sort all the customer records, calculate the age of each customer. Finally, add the version number to handle historical records and output to the data table. Loading: All customer records should be initially loaded into the data warehouse, the new and modified records should be incrementally loaded into the data warehouse with the constructive merge. The final data records which stored in the data warehouse showing as below Figure 3. Figure 3 Customer Dimensional Table 2.2 Product Dimension Table Figure 4 Product ETL Process Extraction: Product and Product Order are the source files in the Inventory System. The mapping relationship is shown in Figure 5. The integration challenge is removing a large number of null rows exist in Product Order table and join these two tables. The extraction of this table is weekly. Figure 5 Product source-target Data Mapping Transformation: the first step is to remove the null rows by filtering those rows whose partNumber field is null. Then retrieve the unitPrice and validFromDate fields from Product Table, it is necessary to extract the year number of orderDate and validFromDate to retrieve to unitPrice for each Product. However, the unitPrice of orders in the year 2019 is not updated, so it remains the same with the unitPrice in 2018. A filter is needed to separate the records in 2019 and set their year number to 2019, then retrieve the corresponding unitPrice. Loading: product records that already exist should be initially loaded into the data warehouse. The new records should be incrementally appended into the data warehouse. The sample of data records is shown in Figure 6. Figure 6 Product Dimensional Table 2.3 Employee Dimension Table Figure 7 Employee ETL Process Extraction: the source file in Sales System is SalesPerson. The mapping between source data and extracted data show is shown in Figure 8. The only difference of the data format is Commission rate, which is String in source field but should be DOUBLE in DW for the convenience of calculation. The table should be updated weekly as a result of the possibility of frequent employee changing. Figure 8 Employee source-target Data Mapping Transformation: the first step is to do Splitting of the commissionRate to keep the number and get rid of the percent sign. Then use a Formula to convert the integer into two decimal places. The last step is to make sure the data in this field is accurate to two decimal places. Loading: all existed records should be initially loaded into the data warehouse. The modified records should be incrementally loaded into the data warehouse with the destructive merge. The sample of data records is shown in Figure 9. Figure 9 Employee Dimensional Table 2.4 Store Dimension Table Figure 10 Store ETL Process Extraction: the source file in the Sales System is Store. The mapping between source data and extracted data show is shown in Figure 11. The data format remains the same. The table should be updated weekly. Figure 11 Store source-target Data Mapping Transformation: no special transformation is needed in this step. Loading: current store records should be initially loaded into the data warehouse. Destructive merge is needed to deal with address changing or opening of stores. The sample of data records is shown in Figure 12. Figure 12 Store Dimensional Table 2.5 Date Dimension Table Figure 13 Date ETL Process Extraction: the date dimension extract data from the data xlsx file. The mapping between source data and target data is addressed in the following Figure 14. There is a data quality issue that the day in the Date field of some records not match with the corresponding Day fields. The extraction frequency should take place daily. Figure 14 Date source-target Data Mapping Transformation: After extracting the source data, the data type should be transferred as predefined in the data warehouse. Then the incorrect date value in Date field should be corrected by a formula that calculates the correct date based on the correct fields including Day, Month and Year. Loading: The existed date records should be initially loaded into the data warehouse. The new records can be loaded daily in the append way. The final result is shown as figure 15. Figure 15 Date Dimensional Table 2.6 Sales Fact Table Figure 16 Sales ETL Process Extraction: the Sales fact table extract data from Order and OrderItem Sources file, also collaborate with the related dimensional table to mapping some of the source fields to the foreign key. The mapping between source data and target data is demonstrated in the following Figure 17. Besides, the extraction is executed weekly. Figure 17 Sales source-target Data Mapping Transformation: the first step is to combine the Order and OrderItem files. Then convert the letter case and Order Date format so as to extract storeID and dateID, which acted as the foreign key, from the store table and data table respectively. Next, change the data type to the format that suitable for later calculation. After that, the database lookup step is used to extract the unitCost from the Product table. The calculator step is then used to calculate the dollarSales and margin. Finally, change the data type to the predefined format when designing a data warehouse. Loading: load all the existed records to the data warehouse initially. Then gradually load the incremental records with the append method. The final result is shown in the following figure 18. Figure 18 Sales Fact Table 2.7 Employees Fact Table Figure 19 Employees ETL Process Extraction: the source data are from Sales Fact Table and Employee Dimension Table. The ETL steps of Sales Fact Table have already provided the Employee_employeeID and Date_dateID within each sale record. For the convenience of tracking the each-day-commission of employees, the table should be sorted by date. The table should be updated monthly. Figure 20 Employees source-target Data Mapping Transformation: select the used part of fields is needed in Employees table, including employeeID, dateID and dollarSales. After sorting these records by date, group the records from the same employee and same date. The dollarSales in each group needs to be summed up. Use employeeID to retrieve the commissionRate of each employee from Employee table, then the total commission is easily derived by multiplying commissionRate and dollarSales. Loading: the existing records will be initially loaded into the data warehouse. New records of Employees is better to be appended to the table without any overwriting. The sample of data records is shown in Figure 21. Figure 21 Employees Fact Table 3 Redesign of the Data Warehouse Change Employee table from the dimension table to the fact table Add ‘orderID’ and ‘lineID’’ attributes in the Sales fact table Integrate the inventory fact table with Product dimension Figure 22 illustrates the redesign of the Fantastic Firework data warehouse, the detailed modification will be addressed in the following paragraphs. Figure 22 Redesign of Data Warehouse In the first version of the design, there is a dimension table called Employee to help measure the key employees in the Sales fact table. In order to better measure the key employees on the basis of commission, an Employee fact table is created and the detailed design is listed below. Employee Fact Table:Attributes: ID, employeeID, dateID, commissionThe Employee Fact Table uses ID as the surrogate key to identify the records uniquely. It has two foreign keys, employeeID and dateID, in order to connect with Employee dimension table and Date dimension table and get commissionRate as well as time data. The commission is calculated by multiplying the commissionRate and dollarSales. Sales Fact Table:In terms of the Sales fact table, the surrogate key ‘saleID’ is replaced by OrderID and LineID in order to better match and track the source data from the original system. Finally, different from the first version, the Inventory Fact Table is integrated into Product Dimension Table, because it is unnecessary to create a Fact Table for Inventory System. 4 Data DictionaryThere are three types of metadata, including operational metadata, business metadata, and technical metadata. Operational metadata provides the schema of data tables in data warehouse. Business metadata shows the data sources and the mapping relationship between source data to the target data, which has been illustrated in the source-target mapping tables in part two. Technical metadata describes the structural information of the target data table, the extraction and loading rules of the data warehouse. 4.1 Customer table metadata Operational: custCode (VARCHAR45), postcode (VARCHAR45), name (VARCHAR45), dob (DATE), validUntilDate (DATE), age (INT), version (INT). Age field is used to support age group query in data warehouse. Version field is used to handle historical records. Business: Source Systems: Sales System (Customer-Brisbane.txt, Customer-Darwin.txt, Customer-Melbourne.txt, Customer-Sydney.txt) Technical: Primary key: custCode, postcodeExtraction rules: extract the new and modified records weeklyLoading rules: initial load and incremental load new record with constructive merge 4.2 Product table metadata Operational: partNumber (VARCHAR 45), validFromDate (DATE), orderDate(DATE), description (VARCHAR100), unitPrice (DOUBLE), quantity (INT), unitCost (DOUBLE). Business: Source Systems: Inventory System (Product.csv, ProductOrder.csv) Technical: Primary key: partNumber, validFromDate, orderDateExtraction rules: extract new records weekly Loading rules: initial load and incremental appending 4.3 Employee table metadata: Operational: employeeID (VARCHAR45), name (VARCHAR45), commissionRate (DOUBLE). Business: Source System: Sales System (SalesPerson.csv) Technical: Primary key: employeeID Extraction rules: extract new records and modify exist records weekly Loading rules: initial load and incremental load with destructive merge 4.4 Store table metadata: Operational: storeID (INT), description (VARCHAR45), address (VARCHAR45). Business: Source System: Sales System (Store.csv) Technical: Primary key: storeID Extraction rules: extract new records and modify exist records weekly Loading rules: initial load and incremental load with destructive merge 4.5 Date table metadata: Operational:dataID(INT), Date(DATE), DayofWeek(VARCHAR10), Month(VARCHAR10), Quarter(INT), Year(INT), Day(INT), Season(VARCHAR10). Business:Source System: data.xlsx Technical:Primary key: dateID Extraction rules: extract new records daily Loading rules: initial load and append new record daily 4.6 Sales table metadata: Operational:OrderID(INT), LineID(INT), Product_partNumber(VARCHAR45), unitSales(INT), salePrice(DOUBLE), Customer_custCode(VARCHAR45), Date_dateID(INT), Store_storeID(INT), Employee_employee(VARCHAR45), dollarSales(DOUBLE), margin(DOUBLE). dollarSales is used to calculate the sales of each item line in order and margin shows the profit. Business:Source System: Sales System(Order.csv, OrderItem.csv) Technical:Primary key: OrderID, LineIDForeign key: Product_partNumber, Customer_xustCode, Date_dataID, Store_storeID, Employee_employeeIDExtraction rules: extract new records weeklyLoading rules: initial load and incremental appending. 4.7 Employees table metadata: Operational: ID (INT), Employee_employeeID (VARCHAR45), Date_dateID (INT), commission (DOUBLE). ID is an auto-generated as a surrogate key. Commission is derived by calculating the commissionRate and dollarSales. Business: Source System: Sales System (Order.csv, OrderItem.csv, SalesPerson.csv) Technical: Primary key: ID Foreign key: Employee_employeeID, Date_dateID Extraction rules: extract new records monthlyLoading rules: initial load and incremental appending","categories":[{"name":"数据","slug":"数据","permalink":"http://yoursite.com/categories/数据/"},{"name":"项目","slug":"数据/项目","permalink":"http://yoursite.com/categories/数据/项目/"},{"name":"Fantastic Fireworks","slug":"数据/项目/Fantastic-Fireworks","permalink":"http://yoursite.com/categories/数据/项目/Fantastic-Fireworks/"}],"tags":[{"name":"数据仓储","slug":"数据仓储","permalink":"http://yoursite.com/tags/数据仓储/"},{"name":"数据","slug":"数据","permalink":"http://yoursite.com/tags/数据/"}],"keywords":[{"name":"数据","slug":"数据","permalink":"http://yoursite.com/categories/数据/"},{"name":"项目","slug":"数据/项目","permalink":"http://yoursite.com/categories/数据/项目/"},{"name":"Fantastic Fireworks","slug":"数据/项目/Fantastic-Fireworks","permalink":"http://yoursite.com/categories/数据/项目/Fantastic-Fireworks/"}]},{"title":"Fantastic Fireworks Data Warehousing Schema Design","slug":"Fantastic-Fireworks-Data-Warehousing-Schema-Design","date":"2019-02-17T11:51:10.000Z","updated":"2019-06-09T16:48:28.749Z","comments":false,"path":"2019/02/17/Fantastic-Fireworks-Data-Warehousing-Schema-Design/","link":"","permalink":"http://yoursite.com/2019/02/17/Fantastic-Fireworks-Data-Warehousing-Schema-Design/","excerpt":"","text":"1 Executive SummaryFantastic Fireworks is a successful enterprise organization which continues to evolve through smart management and strategic vision. As a modern enterprise in the current business world, it requires the necessary strategy information to help make business decisions and compete commercially with other companies. After a general understanding of Fantastic Fireworks, a few conclusions can be drawn: The management of data could be difficult. Fantastic Fireworks uses individual customer table for each store rather than a unified customer table. Also, it uses two separate systems, the inventory system, and sales system. These conditions make data integration and aggregation more difficult, and to some extent, will have a negative impact on the operation of the company. It is necessary to store historical data. Data like the old addresses of customers would still be valuable for analytic use even if they are not up-to-date information. This data is crucial to the statistics of the best-selling products and the company’s most profitable customers. As a result, the data housing solution must provide a solution that can store historical data. A large number of transactions are processed at Fantastic Fireworks every day. In order to improve the efficiency of daily operating and save costs, Fantastic Fireworks needs the system to provide a more convenient way to query data. By adopting the data warehousing solution, these problems would be solved correspondingly: As a single repository of organization data, the data warehouse has natural advantages for data integration and aggregation. The same type of data stored in different stores and systems will be converted into a uniform format to increase data consolidation. A data warehouse has many ways of handling historical information. By introducing version numbers for the same customer, the system will be able to more easily track that customer’s address. Data warehouse makes data available for users and easy to query. Like normal databases, its data could be queried. However, data warehouses are more intuitive and friendly to users without IT support. It allows users to run queries and get results online. 2 Data Warehousing OverviewData warehouses are systems which are used to help organizations do decision-making and acquire strategic information. A data warehouse is a collection of strategies that provide all types of data support for the decision-making process of the enterprise. Different from normal databases, which focus on day to day operations, data warehouses focus more on helping users with revenue analysis and market segmentation. To have a better understanding of data warehousing, a few concepts need to be introduced. Data mart: data mart is a subset of data warehouse, which represents a single business process. Different from the enterprise-width depth of data warehouse, a data mart is often controlled by a single department. In general, databases are used to record transactions, which means they are friendly to updating but not reading. In this case, data marts are necessary to be implemented for the convenience of users querying. OLAP &amp; OLTP: data processing can be divided into two broad categories: online analytical processing (OLAP) and online transaction processing (OLTP). OLTP mainly focuses on basic and daily transaction like bank transactions. The main application of OLTP is the use of databases. In an OLTP system, the memory efficiency of a database is the key, because there is a large number of transactions is processed per second. OLAP is the primary application of data warehouse system. OLAP supports complex analytical operations, focuses on decision support, and provides intuitive query results. Data warehouses, the main application of OLAP, are targeted for decision support. Historical, summarized and consolidated data is more important than detailed, individual records (Chaudhuri, S., &amp; Dayal, U. 1997). With data warehouse and OLAP, the decision makers and senior managers can make better decisions based on strategic information that can be relied on. Dimensional modeling (star): dimensional modeling correlated with Bottom Up approach, one of the approaches to design a data warehouse. It is a design technique to identify and implement business processes. In dimensional modeling, business processes are measured and described from different dimensions. The most core business transaction will be placed in a table called fact table. A row in a fact table corresponds to a measurement. A measurement is a row in a fact table. All the measurements in a fact table must be at the same grain. (Kimball, R., &amp; Ross, M. 2011) There are other tables, called dimension tables, used to help describe the business event from different dimensions. There are many attributes contained in a dimension table, which are used to describe the rows in the table. Dimension tables are the entry points into the fact table. Robust dimension attributes deliver robust analytic slicing and dicing capabilities. The dimensions implement the user interface to the data warehouse. (Kimball, R., &amp; Ross, M. 2011) In general, data warehousing is the foundation of successful business intelligence. It provides a single repository for analytical use and decision making. By using data warehouses, managers will no longer make business decisions with limited data and their intuition. Besides, data warehouses allow users to query data without IT supports, which saves more time and money. The historical data in the data warehouse enables the enterprise to predict and evaluate the future through the analysis of different periods. 3 Dimensional ModelThis part will firstly state the methodology and architecture that used to design the data warehouse. Then it will demonstrate and justify the process and consideration in data warehouse design. 3.1 Designing Methodology and ArchitectureIn this project, the Bottom Up approach is adopted to design the data warehouse. The requirements of Fantastic Fireworks contains the business processes as well as the required source data samples that are used in building the data warehouse schema model. Then the design is started with a single business process, analyzing its measurement and granularity, defining its assist dimensions. This analyzing process is iterated with all other business processes. Finally, these business processes are joint together by connecting the conformed dimensions to generate the whole view of the data warehouse model. Considering the choice of the Bottom Up design methodology, the data warehouse was conducted on the basis of the Data Mart Bus architecture which is highly compatible with Kimball’s approach. With regard to this architecture, we choose one of the business subjects from the real business environment as the supermart, then we gradually combine other data marts by conforming the common dimensions. 3.2 Star Schema DesignA star schema comprises one or more centric fact tables and each fact table links its relevant dimension tables. The star metaphor treats the fact table as the “center” of the star and the dimension table as the “point” (Moody &amp; Kortink, 2003). More specifically, the fact table corresponds to a certain business process, while the dimension tables are responsible for providing the analyzing aspects, considered as the granularity. The dimensional modeling consists of four steps, including the finding the business process; declare the granularity; defining the dimension tables and defining the fact tables. Figure 1 shows the star schema of the Fantastic Firework data warehouse. The design of this star schema will be discussed and justified with these four design steps. Figure 1 The Star Schema of Fantastic Firework Data Warehouse 3.2.1 Business ProcessesAccording to the business process of the Fantastic Fireworks company, there are two main business subjects in its business domain, including the sales and inventory. Besides, the data sources of the data warehouse are stored in the sales system and inventory system respectively, which should take into consideration when unifying the data format in the conformed dimensions. 3.2.2 GranularityGranularity addresses the lowest level of detailed information stored in the data warehouse. It is used to slice the information cube to satisfy the different information demands of end users. Consequently, different fact tables may have different granularities. Regarding the Fantastic Firework project, the sales and inventory are two main business processes(fact tables) of the data warehouse. Referring to the business requirements, the sales answers the question about which consumer has purchased how many products in which store and was served by which employee within a given time. Considering the inventory business event, it answers the question about which product has been ordered how many at a specific point of time. Therefore, the consumer, product, store, employee, and date are the identified granularity of the sales process. The product and date is the required granularity of the inventory process. 3.2.3 Dimension TablesAs we mentioned above, the dimension table is used to provide the fact tables with multiple analyzing aspects to support multiple detailed levels of information queries. Consequently, we can extract dimension tables from the granularities required by the related business processes. Figure 2 demonstrates the dimensions of each business processes and the common dimension between these two processes. Figure 2 Bus Matrix of Fantastic Firework Data Warehouse It is clear that the sales has five analyzing dimensions including consumer, product, store, employee as well as date, while the inventory just comprises product and date dimensions. It is worth mentioning that these two business processes have two common product dimensions. When designing the data structure of each dimension table, it is important to define the appropriate metadata to integrate the source data and only load the data that is needed. The following paragraphs discuss the detailed design and consideration of each dimension tables with referring to Figure 1. The indexes that are used to support fast query in each dimensional table will be discussed in detail in Appendix 2. Consumer Dimension Table Attributes: consumerID, validUnitlDate, name, dob, age, postcode Index: primary(consumerID, validUntilDate), columnstore(age) The consumer might change the address(postcode), so it is a slowly changing dimension. The validUntilDate attribute in the composite primary key can be used to distinguish customers’ valid postcode in different periods and store all the history records. Besides, the current four stores have the four independent consumer tables, which can be integrated into a single customer table in the data warehouse. Apart from the existed attributed in the original system, the age field is added to the consumer dimension table as the summarized information of the customer that can assist in grouping customer by age. Product Dimension Table Attributes: partNumber, validFromDate, description, unitPrice Index: primary(partNumber, validFromDate) The product dimension is the conformed dimension between the sales process and inventory process. It integrates the product table in the inventory system together with the product price list table in the sales system. Considering the unit price of the product might vary sometimes, the validFromDate field is used to distinguish the current price with the historical prices. Besides, the natural key including the attributes of partNumber and the validFromDate act as the composite primary key in the product dimension table. Store Dimension Table Attributes: storeID, description, address Index: primary(storeID) In the store dimension table, we directly use the natural key of the original database as the primary key and each row in the table identifies a specific store. Employee Dimension Table Attributes: employeeID, name, commissionRate Index: primary(employeeID) We directly use the natural key in our employee dimension table to distinguish employees. Each row in this table stands for a sales person. Date Dimension Table Attributes: dateID, Day, Week, Month, Quarter, Year Index: primary(dateID), column(Day/Week/Month/Quarter/Year), clustered(Day, Week, Month, Quarter, Year) The date dimension table is the basic table in almost all data warehouses. The dateID is the surrogate key in the data warehouse. We use the rest attributes to identify a time point to customizable analyzing facts of sales and inventory processes. 3.2.4 Fact TablesAs we mentioned above, the fact table corresponds to the relevant business processes. Therefore, this data warehouse consists of two fact tables including the sales fact table and the inventory fact table. Each fact table comprises the business measurements which used to provide the valuable summarized information that can support decision making. The fact table also contains the foreign keys that link with the dimension tables to do information slicing and selection. Sales Fact Table Attributes: saleID, Employee_employeeID, Consumer_consumerID, Consumer_validUntilDate, Product_partNumber, Product_validFromDate, Date_dataID, Store_storeID, unitSales, dollarSales, margin, commission Index: primary(saleID), columnstore(margin, commission, unitSales), foreign key indexes The sales fact table integrates the sales table and sales item table in the original source database. The saleID is used as the surrogate key of sales fact table rather than the oderID and line number in the original system. The sales fact table contains the additive measurements of unitSales, dollarSales, margin and commission which can be accumulated among all linked dimensions. In addition, there are five foreign keys that link to the five dimension tables respectively to do information filter and selection around the specific customer, employee, store, product and purchase time in sale records. Inventory Fact Table Attributes: inventoryID, Date_dateID, Product_partNumber, Product_validFromDate, productDescription, quantity, unitCost Index: primary(inventoryID), foreign key indexes The inventory fact table uses the inventoryID as the surrogate key to identify the inventory record uniquely. It has two foreign keys to connect with the product dimension table as well as date dimension table so as to assist the analysis of the inventory orders with a given product at a given time. 4 Key DecisionsTo begin with, there are some key attributes in the following sections that may cause misunderstanding. In order to provide a detailed explanation, some Data Dictionary Tables are given in Appendix 1. 4.1 Key CustomersIn order to identify the key customers and their age groups in a given time period, the data warehouse should provide information about the margin, unitSales, dollarSales, dateID (date dimension), customerID (customer dimension), age and postcode. In this case, the data that need to be concerned most are margin, unitSales, dollarSales measurements (in Sales Table) and all attributes in the Customer table. Some other corresponding attributes are in date dimension. Additionally, the summarized information including the dollarSales and margin of each sales record are calculated at the data load stage. The following steps demonstrate how to locate the key customers. Select all the sales records and the required information by joint the sales, customer and date table, in the given time period (e.g. All records in last year) Group the selected records which share the same consumerID Calculate the total margin, unitSales, dollarSales in each customer group Sort these group with one of the three measurements (margin, unitSales, dollarSallers) in descending order with other required information The top x sort results in the rank list can be used as the top x profitable customers. These top x customers can be grouped with the different age grades With the results of key customers, it would be possible for the Fantastic Fireworks to make business decisions. The age attribute in Customer table can be used to analyze the age distribution of the top profitable consumer. With the margin ranking, Fantastic Firework can find out the customers who bring the most profit. Also, to choose the city of new stores, some centralized user clusters must be determined. With the help of the postcode (in Customer table) and margin (in Sales table), it is easy to identify the potential cities that has the high-margin customers. These cities would be more suitable for new stores. 4.2 Most Profitable ProductThose most profitable products in a time period can be elicited by comparing the accumulated margin, unitSales, dollarSales of different products in the given time granularity (monthly, quarterly, yearly). In this case, partNumber (product dimension), dateID (date dimension), margin, unitSales and dollarSales attributes (in Sales table) are most concerned. The same calculating method as before, the margin attribute is derived by subtracting Cost (unitCost*unitSales) from dollarSales. This calculation is accomplished at the data load stage. Select all the sales records within the given time period Group the selected sales records that have the same productNumber Calculate the total margin, unitSales, dollarSales in each product group Sort these groups with one of the three measurement (margin, unitSales, dollarSales) in descending order together with the rest measurements, the productID and given time period The most profitable product is at the top of the rank lit in the given time period With the sorting list of profitable products, Fantastic Fireworks could make business decisions, like putting those highly profitable products in prominent places in the store in the particular month of the year. 4.3 Most Profitable StoreBased on the storeID (store dimension), dateID (date dimension), unitSales, dollarSales and margin attributes (in Sales table), the most profitable store in the given time period could be easily derived. Select all the sales records between the given time period (monthly in the previous year) Group the selected sales records which share the same storeID Calculate the total margin, total unitSales, total dollarSales in each group Sort the group with calculated margin (or unitSales or dollarSales) in descending order together with the rest summarized measurements, the given time period and storeID The most profitable store in each month of the previous year is on the front of the sorting list Some business decisions could be made with the help of these results. it would be wise to expand the floor space of those stores that own more sale records, earn more profits or have more dollar sales. 4.4 Most Profitable Time PeriodThe most profitable time period can be measured by adding the margin of all sales records with which the date associated with the dateID is located within the given time period. To identify the most profitable time period, the dateID (date dimension) and the margin in the sales table should be concerned. Grouping the sales records with a given time granularity(weekday, monthly, quarterly or yearly) Calculate the total margin in each group Sorting the calculated margin and the corresponding time granularity in descending order, and the most profitable time should list in the front of the result. With the help of busy time period like seasons, weekdays and monthly, Fantastic Fireworks is able to optimize store opening time more reasonable. Also, according to the current stocking strategy, most of the reordering decisions are made automatically. In order to prevent the sudden shortage of products, the manager of the inventory system could adjust the reordering decisions with the consideration of the busy time period. 4.5 Key EmployeesThe key employees could be measured by the commission or by the number of served sales records in a given time period. To identify the key employees in a specific time period, the commission (in Sales table), dateID (date dimension) and employeeID (employee dimension) should be taken into consideration. The commission in each record is calculated by dollarSales (in Sales table) multiply the corresponding commissionRate (in Employee table) according to the employeeID. Select all the sales records from sales fact table within the given time period Group the selected sales records which share the same employeeID Calculate the total commission in each group as the total commission of that employee in the given time period Sort the calculated commission with the corresponding employee information with descend order. The key employees would be seen in the front rank. The approach to get the key employee who owns the most sales records can be elicited similarly by count the number of records in each group. With these two different measurements, Fantastic Fireworks is able to evaluate its employees in different ways. Further strategies of staff management would be adjusted based on that. Appendix 1: Data Dictionary Product Table Figure 3 Product Table Inventory Table Figure 4 Inventory Table Sales Table Figure 5 Sales Table Customer Table Figure 6 Customer Table Employee Table Figure 7 Employee Table Store Table Figure 8 Store Table Date Table Figure 9 Date Table Appendix 2: SQL Statements123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0;SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0;SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=&apos;ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION&apos;;-- ------------------------------------------------------- Schema Fireworks-- ------------------------------------------------------- ------------------------------------------------------- Schema Fireworks-- -----------------------------------------------------CREATE SCHEMA IF NOT EXISTS `Fireworks` DEFAULT CHARACTER SET utf8 ;USE `Fireworks` ;-- ------------------------------------------------------- Table `Fireworks`.`Store`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Store` ( `storeID` INT NOT NULL, `description` VARCHAR(45) NULL, `address` VARCHAR(45) NULL, PRIMARY KEY (`storeID`))ENGINE = InnoDB;-- ------------------------------------------------------- Table `Fireworks`.`Product`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Product` ( `partNumber` VARCHAR(45) NOT NULL, `validFromDate` DATETIME NOT NULL, `description` VARCHAR(45) NULL, `unitPrice` DOUBLE NULL, PRIMARY KEY (`partNumber`, `validFromDate`))ENGINE = InnoDB;-- ------------------------------------------------------- Table `Fireworks`.`Customer`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Customer` ( `customerID` VARCHAR(45) NOT NULL, `validUntilDate` DATETIME NOT NULL, `name` VARCHAR(45) NULL, `dob` DATETIME NULL, `age` INT NULL, `postcode` VARCHAR(45) NULL, PRIMARY KEY (`customerID`, `validUntilDate`), INDEX `age` (`age` ASC) VISIBLE)ENGINE = InnoDB;-- ------------------------------------------------------- Table `Fireworks`.`Employee`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Employee` ( `employeeID` VARCHAR(45) NOT NULL, `name` VARCHAR(45) NULL, `commissionRate` DOUBLE NULL, PRIMARY KEY (`employeeID`))ENGINE = InnoDB;-- ------------------------------------------------------- Table `Fireworks`.`Date`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Date` ( `dateID` INT NOT NULL, `Day` INT NULL, `Week` INT NULL, `Month` INT NULL, `Quarter` INT NULL, `Year` INT NULL, PRIMARY KEY (`dateID`), INDEX `Day` (`Day` ASC) VISIBLE, INDEX `Week` (`Week` ASC) VISIBLE, INDEX `Month` (`Month` ASC) VISIBLE, INDEX `Quarter` (`Quarter` ASC) VISIBLE, INDEX `Year` (`Year` ASC) VISIBLE, INDEX `Date` (`Day` ASC, `Week` ASC, `Month` ASC, `Quarter` ASC, `Year` ASC) VISIBLE)ENGINE = InnoDB;-- ------------------------------------------------------- Table `Fireworks`.`Sales`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Sales` ( `saleID` INT NOT NULL, `Employee_employeeID` VARCHAR(45) NOT NULL, `Date_dateID` INT NOT NULL, `Store_storeID` INT NOT NULL, `Product_partNumber` VARCHAR(45) NOT NULL, `Product_validFromDate` DATETIME NOT NULL, `Customer_customerID` VARCHAR(45) NOT NULL, `Customer_validUntilDate` DATETIME NOT NULL, `unitSales` INT NULL, `salePrice` DOUBLE NULL, `dollarSales` DOUBLE NULL, `margin` DOUBLE NULL, `commission` DOUBLE NULL, PRIMARY KEY (`saleID`), INDEX `fk_Sales_Employee_idx` (`Employee_employeeID` ASC) VISIBLE, INDEX `fk_Sales_Date1_idx` (`Date_dateID` ASC) VISIBLE, INDEX `fk_SalesItem_Store1_idx` (`Store_storeID` ASC) VISIBLE, INDEX `margin` (`margin` ASC) VISIBLE, INDEX `commission` (`commission` ASC) VISIBLE, INDEX `fk_Sales_Product1_idx` (`Product_partNumber` ASC, `Product_validFromDate` ASC) VISIBLE, INDEX `fk_Sales_Customer1_idx` (`Customer_customerID` ASC, `Customer_validUntilDate` ASC) VISIBLE, INDEX `unitSales` (`unitSales` ASC) VISIBLE, CONSTRAINT `fk_Sales_Employee` FOREIGN KEY (`Employee_employeeID`) REFERENCES `Fireworks`.`Employee` (`employeeID`) ON DELETE NO ACTION ON UPDATE NO ACTION, CONSTRAINT `fk_Sales_Date1` FOREIGN KEY (`Date_dateID`) REFERENCES `Fireworks`.`Date` (`dateID`) ON DELETE NO ACTION ON UPDATE NO ACTION, CONSTRAINT `fk_SalesItem_Store1` FOREIGN KEY (`Store_storeID`) REFERENCES `Fireworks`.`Store` (`storeID`) ON DELETE NO ACTION ON UPDATE NO ACTION, CONSTRAINT `fk_Sales_Product1` FOREIGN KEY (`Product_partNumber` , `Product_validFromDate`) REFERENCES `Fireworks`.`Product` (`partNumber` , `validFromDate`) ON DELETE NO ACTION ON UPDATE NO ACTION, CONSTRAINT `fk_Sales_Customer1` FOREIGN KEY (`Customer_customerID` , `Customer_validUntilDate`) REFERENCES `Fireworks`.`Customer` (`customerID` , `validUntilDate`) ON DELETE NO ACTION ON UPDATE NO ACTION)ENGINE = InnoDB;-- ------------------------------------------------------- Table `Fireworks`.`Inventory`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Inventory` ( `inventoryID` INT NOT NULL, `Date_dateID` INT NOT NULL, `Product_partNumber` VARCHAR(45) NOT NULL, `Product_validFromDate` DATETIME NOT NULL, `productDescription` VARCHAR(45) NULL, `quantity` INT NULL, `unitCost` DOUBLE NULL, PRIMARY KEY (`inventoryID`), INDEX `fk_Inventory_Date1_idx` (`Date_dateID` ASC) VISIBLE, INDEX `fk_Inventory_Product1_idx` (`Product_partNumber` ASC, `Product_validFromDate` ASC) VISIBLE, CONSTRAINT `fk_Inventory_Date1` FOREIGN KEY (`Date_dateID`) REFERENCES `Fireworks`.`Date` (`dateID`) ON DELETE NO ACTION ON UPDATE NO ACTION, CONSTRAINT `fk_Inventory_Product1` FOREIGN KEY (`Product_partNumber` , `Product_validFromDate`) REFERENCES `Fireworks`.`Product` (`partNumber` , `validFromDate`) ON DELETE NO ACTION ON UPDATE NO ACTION)ENGINE = InnoDB;SET SQL_MODE=@OLD_SQL_MODE;SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS;SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS; References[1] Chaudhuri, S., &amp; Dayal, U. (1997). An overview of data warehousing and OLAP technology. ACM Sigmod record, 26(1), 65-74. [2] Kimball, R., &amp; Ross, M. (2011). The data warehouse toolkit: the complete guide to dimensional modeling. John Wiley &amp; Sons. [3] Moody, D. L., &amp; Kortink, M. A. R. (2003). From ER models to dimensional models: bridging the gap between OLTP and OLAP design, Part I. Business Intelligence Journal, 8, 7-24.","categories":[{"name":"数据","slug":"数据","permalink":"http://yoursite.com/categories/数据/"},{"name":"项目","slug":"数据/项目","permalink":"http://yoursite.com/categories/数据/项目/"},{"name":"Fantastic Fireworks","slug":"数据/项目/Fantastic-Fireworks","permalink":"http://yoursite.com/categories/数据/项目/Fantastic-Fireworks/"}],"tags":[{"name":"数据仓储","slug":"数据仓储","permalink":"http://yoursite.com/tags/数据仓储/"},{"name":"数据","slug":"数据","permalink":"http://yoursite.com/tags/数据/"}],"keywords":[{"name":"数据","slug":"数据","permalink":"http://yoursite.com/categories/数据/"},{"name":"项目","slug":"数据/项目","permalink":"http://yoursite.com/categories/数据/项目/"},{"name":"Fantastic Fireworks","slug":"数据/项目/Fantastic-Fireworks","permalink":"http://yoursite.com/categories/数据/项目/Fantastic-Fireworks/"}]}]}