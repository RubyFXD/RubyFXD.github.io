{"meta":{"title":"Ruby's Blogs","subtitle":null,"description":null,"author":"RubyFXD","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"Hexo 图片无法显示问题","slug":"Hexo-图片无法显示问题","date":"2019-07-09T04:51:29.000Z","updated":"2019-07-09T06:59:06.369Z","comments":false,"path":"2019/07/09/Hexo-图片无法显示问题/","link":"","permalink":"http://yoursite.com/2019/07/09/Hexo-图片无法显示问题/","excerpt":"","text":"图片不显示问题Hexo的source文件夹下，除了存放博客的md文件，还可以存放博客需要的图片，并通过Markdown引用图片的语法，引用图片到文章。 1![图片名称](图片相对路径) 在启动本地服务器后，本地测试图片显示没有问题。当把文章部署到git之后，远程访问图片会出现不显示的问题。因为图片的相对路径在source文件夹下和public文件夹下的相对路径不一致，public文件夹下会在图片的相对地址前加上年月日文件结构，因此访问不到。 一般有两种解决方法： 资源文件夹 + 相对地址引用的图片标签插件 图床 + 绝对地址 图片标签插件图片存在本地source文件夹下和md文档同名的资源文件夹下，通过使用图片标签插件，使用相对路径引用图片。 1.开启资源文件夹功能打开hexo文件夹根目录下的 _config.yml配置文件，将post_asset_folder设置为true。 123_config.ymlpost_asset_folder: true 在新建文章时，_post文件夹下除了md文档外，还会出现同名文件夹用来存放图片，如下图。 Figure 1 资源文件夹 2.下载图片标签插件安装hexo-asset-image插件，用来自动转换图片路径。 12$ cd hexo博客目录$ npm install https://github.com/CodeFalling/hexo-asset-image --save 3.使用插件引用图片将需要md文章中需要饮用的文件放在同名资源文件夹中，通过标签插件直接引用。格式为{ %asset_img 图片名称.图片格式 [title] %}。标签插件会将自动转化图片的相对路径和public种的日期文件结构一致，解决路径不一致问题。 图床图床是将要引用的图片放在可托管的服务器上，在文章中引用服务器上图片的绝对地址，同样不会造成图片文件路径不一致的问题。 常用图床： 1.七牛云2.SM.MS3.ImgURL4.postimg5.聚合图床","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"}],"keywords":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"}]},{"title":"OSX下使用Hexo和GitHub搭建个人博客","slug":"OSX下使用Hexo和GitHub搭建个人博客","date":"2019-07-06T02:38:05.000Z","updated":"2019-07-09T02:33:15.228Z","comments":false,"path":"2019/07/06/OSX下使用Hexo和GitHub搭建个人博客/","link":"","permalink":"http://yoursite.com/2019/07/06/OSX下使用Hexo和GitHub搭建个人博客/","excerpt":"","text":"一、准备1. 什么是Hexo?Hexo是一个使用node.js开发的命令行脚本工具。Hexo是一个简单、高效的博客框架，可以把Markdown文件翻译成html页面。将Hexo在本地生成的静态网站部署到web服务器上，形成可供访问的个人博客站点。 2. GitHub账号Hexo是一个静态博客框架，产生的html文件及相关文件需要部署到web服务器上。 GitHub Pages是面向用户、组织和项目的公共静态页面搭建托管服务，站点可以被免费托管在GitHub上。GitHub Pages相当于一个web服务器，可以通过git将本地Hexo生成的静态站点上传到GitHub Pages上。 3. node.jsnode.js是一种javascript的运行环境，它把浏览器的解释器封装起来，支持javascript脱离浏览器运行。Hexo是基于node.js开发的工具，需要node.js支持，所以安装Hexo之前确认安装node.js. 4. 环境配置 git+node.js(1) 检查是否安装git和node.js 12$ git --version$ node -v 如果已经安装，跳到二、Hexo安装配置；如果没有安装mac用户可以直接使用Homebrew命令安装。 (2)Homebrew安装 /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 检查Homebrew是否安装成功: $ brew -v (3) git安装： 传送门：git安装及配置 $ brew install git (4) node安装： 传送门： 使用Homebrew安装node.js $ brew install node 查看安装版本 12$ node --version$ npm --version tips: npm是和node.js一起安装的包管理工具，用来管理nodejs的第三方插件，支持用户从npm服务器下载第三方包到本地使用。 二、Hexo安装配置传送门：Hexo官方使用文档 1.使用npm进行Hexo安装$ npm install -g hexo-cli tips: 可能会出现permission denied error，如图1；解决方法如图二。 Figure 1 Hexo Install Error Figure 2 Solution 2. Hexo建站Hexo安装完成后，执行以下命令，Hexo将会在指定文件夹中新建所需要的文件。 1234$ hexo init &lt;folder&gt;e.g. $ hexo init mybolgs$ cd &lt;folder&gt;$ npm install 命令执行后，指定文件夹下的目录结构如下： Figure 3 目录结构 _config.yml是网站的配置文件，包含网站的配置信息 package.json应用程序信息 Scaffolds模版文件夹，新建文章的时候，Hexo会根据scaffold中的模版建立文件。Hexo模版是在新建markdown文件中默认的填充内容 source资源文件夹，存放用户资源。markdown和HTML文件会被解析并放到public文件夹，其他文件（除了被忽略文件）将会被拷贝到puclic文件夹。 Themes主题文件夹，Hexo会根据主题来生成相应的静态页面 3.Hexo站点测试$ hexo server hexo server 命令启动本地Hexo服务器，浏览器中输入: http://localhost:4000/ 如果访问到Hexo Hello World页面，证明 Hexo本地配置成功，个人站点建立成功。 4.Hexo _config.yml网站配置Hexo建站后，可以对_config.yml文件进行修改进行网站配置。修改_config.yml文件可以改变如网站标题，站点目录结构，文章分类标签，时区和日期格式等参数。 如果不进行设置，Hexo将采用_config.yml中默认值。新手可以跳过此步骤，先熟悉如何利用Hexo新建文章，生成html页面及部署等必要步骤。 传送门：Hexo官方_config.yml配置说明 5.Hexo 写博文（1）新建文章 $ hexo new [layout] &lt;title&gt; 该命令用来新建一篇文章，[layout]可以省略，如果没有设置，默认使用 _config.yml中default_layout 参数。layout用来指定生成的页面布局，默认default_layout是post（文章详情页面布局），即生成博文页面的html页面布局。 tips: 如果title标题中有空格，需要双引号扩起来。 e.g. $ hexo new &quot;The First Post Test&quot; 新建文章之后，若是默认的default_layout，则在source/ _posts文件夹下可以看到对应的md文件。 （2）生成Html页面 $ hexo generate 生成静态文件，将post下md文件翻译成对应的html页面（出现在public文件夹下）。 （3）启动本地服务器测试 $ hexo server 访问http://localhost:4000/，可以查看本地生成的个人站点，找到刚生成的文章。 三、Hexo部署到Git1.配置github pages在GitHub上新建一个仓库，命名为账户名.github.io。e.g. zhangsan.github.io github pages使用传送门 2.修改Hexo配置文件关联github pages（1）进入hexo站点文件夹下 12$ cd &lt;folder&gt;e.g. $ cd myblogs (2) 更改站点文件夹下_config.yml文件 1$ vi _config.yml 键入i，修改配置如下图。注意type, repository, branch冒号后面要加上一个空格，否则会报错。修改完成后，按esc,键入:wq保存退出。 Figure 4 _config.yml配置 3.安装插件1$ npm install hexo-deployer-git --save 4.将本地站点部署到GitHub上123456// 清理已经生成的弃用文件$ hexo clean// 生成新的站点$ hexo generate// 将生成的站点推送到GitHub Pages仓库$ hexo deploy 部署完成后，通过浏览器输入 https://账户名.github.io/ 访问你的个人博客。 四、更换主题1. 将主题clone到themes目录下可以在Hexo官方主题库中找到自己喜欢的主题，将主题对应的git仓库clone到本地博客文件夹themes目录下，使用 $ git clone url命令，例如下 12$ cd &lt;博客存放的folder&gt;$ git clone https://github.com/shenliyang/hexo-theme-snippet.git themes/hexo-theme-snippet themes/hexo-theme-snippet指定要将该主题clone到themes的hexo-theme-snippet文件下，效果如下图 Figure 5 clone主题 2. 修改配置文件 Figure 6 修改配置文件 圈1: Hexo根目录下的配置文件 _config.yml是用来配置整个站点的。打开此配置文件，修改主题。 theme: hexo-theme-snippet 圈2: Hexo/themes/某个主题文件夹 下的 _config.yml是用来配置这个主题的相关参数的，根据你clone的主题进行具体配置，作者大大写的更加详细～ snippet主题配置传送门。 3. 清理并重新生成站点更换主题之后，可以将之前主题生成的静态文件清理，然后根据当前主题重新生成站点，并部署。 123$ hexo clean$ hexo generate$ hexo deploy","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}],"keywords":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"}]},{"title":"Fantastic Fireworks Data Warehousing ETL Process Design","slug":"Fantastic-Fireworks-Data-Warehousing-ETL-Process-Design","date":"2019-03-25T15:50:13.000Z","updated":"2019-06-10T03:40:30.892Z","comments":false,"path":"2019/03/26/Fantastic-Fireworks-Data-Warehousing-ETL-Process-Design/","link":"","permalink":"http://yoursite.com/2019/03/26/Fantastic-Fireworks-Data-Warehousing-ETL-Process-Design/","excerpt":"","text":"1 Executive SummaryIn the era of the information explosion, the data warehouse can better assist the enterprises to manage and analyze their business data, so as to support decision-making. After modeling the star schema for the Fantastic Firework data warehouse, the next step is to pop the source data which may be stored in several independent systems into the designed data warehouse. The data integration and data quality are the critical issues in this step. This report will refine the previous star schema. Additionally, the report will discuss the design of the ETL process which is used to extract the raw data from the operational systems, transform the raw data to the unified data format to ensure the data quality, and load the processed data to the data warehouse. The data dictionary will be attached to provide the user with the definition of the schema and data content of the data warehouse, also it will navigate the end user where the integrated data is sourced from. Issues in ETL process design: The difference of date type between source systems, need to be unified in the data warehouse. Handle low data quality records and invalid inputs (like replacing letter “o” with number “0” in Customer table). Data integration across systems (like retrieving the cost per item from Inventory System to sales table). 2 Design of the ETL Process2.1 Customer Dimension Table Figure 1 Customer ETL Process Extraction: extract data from four source customer datasets. The mapping between source data and extracted data is shown in Figure 2. There are some data issues when extracting data from multiple data sources, including different data type with the same field; misspelled postcode(number 0 and letter O, incorrect length), detect the same customer in different stores. The extraction is executed weekly, the version field is used to handle the historical customer records. Figure 2 Customer source-target Data Mapping Transformation: The first step is to unify the field name and data type after reading from the four source files. Then replace the letter O to number 0 in postcode and handle the misspelled postcode with incorrect postcode length. Meanwhile, detect and merge the same customer record between Darwin and Melbourne store. After that, merge and sort all the customer records, calculate the age of each customer. Finally, add the version number to handle historical records and output to the data table. Loading: All customer records should be initially loaded into the data warehouse, the new and modified records should be incrementally loaded into the data warehouse with the constructive merge. The final data records which stored in the data warehouse showing as below Figure 3. Figure 3 Customer Dimensional Table 2.2 Product Dimension Table Figure 4 Product ETL Process Extraction: Product and Product Order are the source files in the Inventory System. The mapping relationship is shown in Figure 5. The integration challenge is removing a large number of null rows exist in Product Order table and join these two tables. The extraction of this table is weekly. Figure 5 Product source-target Data Mapping Transformation: the first step is to remove the null rows by filtering those rows whose partNumber field is null. Then retrieve the unitPrice and validFromDate fields from Product Table, it is necessary to extract the year number of orderDate and validFromDate to retrieve to unitPrice for each Product. However, the unitPrice of orders in the year 2019 is not updated, so it remains the same with the unitPrice in 2018. A filter is needed to separate the records in 2019 and set their year number to 2019, then retrieve the corresponding unitPrice. Loading: product records that already exist should be initially loaded into the data warehouse. The new records should be incrementally appended into the data warehouse. The sample of data records is shown in Figure 6. Figure 6 Product Dimensional Table 2.3 Employee Dimension Table Figure 7 Employee ETL Process Extraction: the source file in Sales System is SalesPerson. The mapping between source data and extracted data show is shown in Figure 8. The only difference of the data format is Commission rate, which is String in source field but should be DOUBLE in DW for the convenience of calculation. The table should be updated weekly as a result of the possibility of frequent employee changing. Figure 8 Employee source-target Data Mapping Transformation: the first step is to do Splitting of the commissionRate to keep the number and get rid of the percent sign. Then use a Formula to convert the integer into two decimal places. The last step is to make sure the data in this field is accurate to two decimal places. Loading: all existed records should be initially loaded into the data warehouse. The modified records should be incrementally loaded into the data warehouse with the destructive merge. The sample of data records is shown in Figure 9. Figure 9 Employee Dimensional Table 2.4 Store Dimension Table Figure 10 Store ETL Process Extraction: the source file in the Sales System is Store. The mapping between source data and extracted data show is shown in Figure 11. The data format remains the same. The table should be updated weekly. Figure 11 Store source-target Data Mapping Transformation: no special transformation is needed in this step. Loading: current store records should be initially loaded into the data warehouse. Destructive merge is needed to deal with address changing or opening of stores. The sample of data records is shown in Figure 12. Figure 12 Store Dimensional Table 2.5 Date Dimension Table Figure 13 Date ETL Process Extraction: the date dimension extract data from the data xlsx file. The mapping between source data and target data is addressed in the following Figure 14. There is a data quality issue that the day in the Date field of some records not match with the corresponding Day fields. The extraction frequency should take place daily. Figure 14 Date source-target Data Mapping Transformation: After extracting the source data, the data type should be transferred as predefined in the data warehouse. Then the incorrect date value in Date field should be corrected by a formula that calculates the correct date based on the correct fields including Day, Month and Year. Loading: The existed date records should be initially loaded into the data warehouse. The new records can be loaded daily in the append way. The final result is shown as figure 15. Figure 15 Date Dimensional Table 2.6 Sales Fact Table Figure 16 Sales ETL Process Extraction: the Sales fact table extract data from Order and OrderItem Sources file, also collaborate with the related dimensional table to mapping some of the source fields to the foreign key. The mapping between source data and target data is demonstrated in the following Figure 17. Besides, the extraction is executed weekly. Figure 17 Sales source-target Data Mapping Transformation: the first step is to combine the Order and OrderItem files. Then convert the letter case and Order Date format so as to extract storeID and dateID, which acted as the foreign key, from the store table and data table respectively. Next, change the data type to the format that suitable for later calculation. After that, the database lookup step is used to extract the unitCost from the Product table. The calculator step is then used to calculate the dollarSales and margin. Finally, change the data type to the predefined format when designing a data warehouse. Loading: load all the existed records to the data warehouse initially. Then gradually load the incremental records with the append method. The final result is shown in the following figure 18. Figure 18 Sales Fact Table 2.7 Employees Fact Table Figure 19 Employees ETL Process Extraction: the source data are from Sales Fact Table and Employee Dimension Table. The ETL steps of Sales Fact Table have already provided the Employee_employeeID and Date_dateID within each sale record. For the convenience of tracking the each-day-commission of employees, the table should be sorted by date. The table should be updated monthly. Figure 20 Employees source-target Data Mapping Transformation: select the used part of fields is needed in Employees table, including employeeID, dateID and dollarSales. After sorting these records by date, group the records from the same employee and same date. The dollarSales in each group needs to be summed up. Use employeeID to retrieve the commissionRate of each employee from Employee table, then the total commission is easily derived by multiplying commissionRate and dollarSales. Loading: the existing records will be initially loaded into the data warehouse. New records of Employees is better to be appended to the table without any overwriting. The sample of data records is shown in Figure 21. Figure 21 Employees Fact Table 3 Redesign of the Data Warehouse Change Employee table from the dimension table to the fact table Add ‘orderID’ and ‘lineID’’ attributes in the Sales fact table Integrate the inventory fact table with Product dimension Figure 22 illustrates the redesign of the Fantastic Firework data warehouse, the detailed modification will be addressed in the following paragraphs. Figure 22 Redesign of Data Warehouse In the first version of the design, there is a dimension table called Employee to help measure the key employees in the Sales fact table. In order to better measure the key employees on the basis of commission, an Employee fact table is created and the detailed design is listed below. Employee Fact Table:Attributes: ID, employeeID, dateID, commissionThe Employee Fact Table uses ID as the surrogate key to identify the records uniquely. It has two foreign keys, employeeID and dateID, in order to connect with Employee dimension table and Date dimension table and get commissionRate as well as time data. The commission is calculated by multiplying the commissionRate and dollarSales. Sales Fact Table:In terms of the Sales fact table, the surrogate key ‘saleID’ is replaced by OrderID and LineID in order to better match and track the source data from the original system. Finally, different from the first version, the Inventory Fact Table is integrated into Product Dimension Table, because it is unnecessary to create a Fact Table for Inventory System. 4 Data DictionaryThere are three types of metadata, including operational metadata, business metadata, and technical metadata. Operational metadata provides the schema of data tables in data warehouse. Business metadata shows the data sources and the mapping relationship between source data to the target data, which has been illustrated in the source-target mapping tables in part two. Technical metadata describes the structural information of the target data table, the extraction and loading rules of the data warehouse. 4.1 Customer table metadata Operational: custCode (VARCHAR45), postcode (VARCHAR45), name (VARCHAR45), dob (DATE), validUntilDate (DATE), age (INT), version (INT). Age field is used to support age group query in data warehouse. Version field is used to handle historical records. Business: Source Systems: Sales System (Customer-Brisbane.txt, Customer-Darwin.txt, Customer-Melbourne.txt, Customer-Sydney.txt) Technical: Primary key: custCode, postcodeExtraction rules: extract the new and modified records weeklyLoading rules: initial load and incremental load new record with constructive merge 4.2 Product table metadata Operational: partNumber (VARCHAR 45), validFromDate (DATE), orderDate(DATE), description (VARCHAR100), unitPrice (DOUBLE), quantity (INT), unitCost (DOUBLE). Business: Source Systems: Inventory System (Product.csv, ProductOrder.csv) Technical: Primary key: partNumber, validFromDate, orderDateExtraction rules: extract new records weekly Loading rules: initial load and incremental appending 4.3 Employee table metadata: Operational: employeeID (VARCHAR45), name (VARCHAR45), commissionRate (DOUBLE). Business: Source System: Sales System (SalesPerson.csv) Technical: Primary key: employeeID Extraction rules: extract new records and modify exist records weekly Loading rules: initial load and incremental load with destructive merge 4.4 Store table metadata: Operational: storeID (INT), description (VARCHAR45), address (VARCHAR45). Business: Source System: Sales System (Store.csv) Technical: Primary key: storeID Extraction rules: extract new records and modify exist records weekly Loading rules: initial load and incremental load with destructive merge 4.5 Date table metadata: Operational:dataID(INT), Date(DATE), DayofWeek(VARCHAR10), Month(VARCHAR10), Quarter(INT), Year(INT), Day(INT), Season(VARCHAR10). Business:Source System: data.xlsx Technical:Primary key: dateID Extraction rules: extract new records daily Loading rules: initial load and append new record daily 4.6 Sales table metadata: Operational:OrderID(INT), LineID(INT), Product_partNumber(VARCHAR45), unitSales(INT), salePrice(DOUBLE), Customer_custCode(VARCHAR45), Date_dateID(INT), Store_storeID(INT), Employee_employee(VARCHAR45), dollarSales(DOUBLE), margin(DOUBLE). dollarSales is used to calculate the sales of each item line in order and margin shows the profit. Business:Source System: Sales System(Order.csv, OrderItem.csv) Technical:Primary key: OrderID, LineIDForeign key: Product_partNumber, Customer_xustCode, Date_dataID, Store_storeID, Employee_employeeIDExtraction rules: extract new records weeklyLoading rules: initial load and incremental appending. 4.7 Employees table metadata: Operational: ID (INT), Employee_employeeID (VARCHAR45), Date_dateID (INT), commission (DOUBLE). ID is an auto-generated as a surrogate key. Commission is derived by calculating the commissionRate and dollarSales. Business: Source System: Sales System (Order.csv, OrderItem.csv, SalesPerson.csv) Technical: Primary key: ID Foreign key: Employee_employeeID, Date_dateID Extraction rules: extract new records monthlyLoading rules: initial load and incremental appending","categories":[{"name":"数据","slug":"数据","permalink":"http://yoursite.com/categories/数据/"},{"name":"项目","slug":"数据/项目","permalink":"http://yoursite.com/categories/数据/项目/"},{"name":"Fantastic Fireworks","slug":"数据/项目/Fantastic-Fireworks","permalink":"http://yoursite.com/categories/数据/项目/Fantastic-Fireworks/"}],"tags":[{"name":"数据仓储","slug":"数据仓储","permalink":"http://yoursite.com/tags/数据仓储/"},{"name":"数据","slug":"数据","permalink":"http://yoursite.com/tags/数据/"}],"keywords":[{"name":"数据","slug":"数据","permalink":"http://yoursite.com/categories/数据/"},{"name":"项目","slug":"数据/项目","permalink":"http://yoursite.com/categories/数据/项目/"},{"name":"Fantastic Fireworks","slug":"数据/项目/Fantastic-Fireworks","permalink":"http://yoursite.com/categories/数据/项目/Fantastic-Fireworks/"}]},{"title":"Fantastic Fireworks Data Warehousing Schema Design","slug":"Fantastic-Fireworks-Data-Warehousing-Schema-Design","date":"2019-02-17T08:51:10.000Z","updated":"2019-06-09T16:48:28.749Z","comments":false,"path":"2019/02/17/Fantastic-Fireworks-Data-Warehousing-Schema-Design/","link":"","permalink":"http://yoursite.com/2019/02/17/Fantastic-Fireworks-Data-Warehousing-Schema-Design/","excerpt":"","text":"1 Executive SummaryFantastic Fireworks is a successful enterprise organization which continues to evolve through smart management and strategic vision. As a modern enterprise in the current business world, it requires the necessary strategy information to help make business decisions and compete commercially with other companies. After a general understanding of Fantastic Fireworks, a few conclusions can be drawn: The management of data could be difficult. Fantastic Fireworks uses individual customer table for each store rather than a unified customer table. Also, it uses two separate systems, the inventory system, and sales system. These conditions make data integration and aggregation more difficult, and to some extent, will have a negative impact on the operation of the company. It is necessary to store historical data. Data like the old addresses of customers would still be valuable for analytic use even if they are not up-to-date information. This data is crucial to the statistics of the best-selling products and the company’s most profitable customers. As a result, the data housing solution must provide a solution that can store historical data. A large number of transactions are processed at Fantastic Fireworks every day. In order to improve the efficiency of daily operating and save costs, Fantastic Fireworks needs the system to provide a more convenient way to query data. By adopting the data warehousing solution, these problems would be solved correspondingly: As a single repository of organization data, the data warehouse has natural advantages for data integration and aggregation. The same type of data stored in different stores and systems will be converted into a uniform format to increase data consolidation. A data warehouse has many ways of handling historical information. By introducing version numbers for the same customer, the system will be able to more easily track that customer’s address. Data warehouse makes data available for users and easy to query. Like normal databases, its data could be queried. However, data warehouses are more intuitive and friendly to users without IT support. It allows users to run queries and get results online. 2 Data Warehousing OverviewData warehouses are systems which are used to help organizations do decision-making and acquire strategic information. A data warehouse is a collection of strategies that provide all types of data support for the decision-making process of the enterprise. Different from normal databases, which focus on day to day operations, data warehouses focus more on helping users with revenue analysis and market segmentation. To have a better understanding of data warehousing, a few concepts need to be introduced. Data mart: data mart is a subset of data warehouse, which represents a single business process. Different from the enterprise-width depth of data warehouse, a data mart is often controlled by a single department. In general, databases are used to record transactions, which means they are friendly to updating but not reading. In this case, data marts are necessary to be implemented for the convenience of users querying. OLAP &amp; OLTP: data processing can be divided into two broad categories: online analytical processing (OLAP) and online transaction processing (OLTP). OLTP mainly focuses on basic and daily transaction like bank transactions. The main application of OLTP is the use of databases. In an OLTP system, the memory efficiency of a database is the key, because there is a large number of transactions is processed per second. OLAP is the primary application of data warehouse system. OLAP supports complex analytical operations, focuses on decision support, and provides intuitive query results. Data warehouses, the main application of OLAP, are targeted for decision support. Historical, summarized and consolidated data is more important than detailed, individual records (Chaudhuri, S., &amp; Dayal, U. 1997). With data warehouse and OLAP, the decision makers and senior managers can make better decisions based on strategic information that can be relied on. Dimensional modeling (star): dimensional modeling correlated with Bottom Up approach, one of the approaches to design a data warehouse. It is a design technique to identify and implement business processes. In dimensional modeling, business processes are measured and described from different dimensions. The most core business transaction will be placed in a table called fact table. A row in a fact table corresponds to a measurement. A measurement is a row in a fact table. All the measurements in a fact table must be at the same grain. (Kimball, R., &amp; Ross, M. 2011) There are other tables, called dimension tables, used to help describe the business event from different dimensions. There are many attributes contained in a dimension table, which are used to describe the rows in the table. Dimension tables are the entry points into the fact table. Robust dimension attributes deliver robust analytic slicing and dicing capabilities. The dimensions implement the user interface to the data warehouse. (Kimball, R., &amp; Ross, M. 2011) In general, data warehousing is the foundation of successful business intelligence. It provides a single repository for analytical use and decision making. By using data warehouses, managers will no longer make business decisions with limited data and their intuition. Besides, data warehouses allow users to query data without IT supports, which saves more time and money. The historical data in the data warehouse enables the enterprise to predict and evaluate the future through the analysis of different periods. 3 Dimensional ModelThis part will firstly state the methodology and architecture that used to design the data warehouse. Then it will demonstrate and justify the process and consideration in data warehouse design. 3.1 Designing Methodology and ArchitectureIn this project, the Bottom Up approach is adopted to design the data warehouse. The requirements of Fantastic Fireworks contains the business processes as well as the required source data samples that are used in building the data warehouse schema model. Then the design is started with a single business process, analyzing its measurement and granularity, defining its assist dimensions. This analyzing process is iterated with all other business processes. Finally, these business processes are joint together by connecting the conformed dimensions to generate the whole view of the data warehouse model. Considering the choice of the Bottom Up design methodology, the data warehouse was conducted on the basis of the Data Mart Bus architecture which is highly compatible with Kimball’s approach. With regard to this architecture, we choose one of the business subjects from the real business environment as the supermart, then we gradually combine other data marts by conforming the common dimensions. 3.2 Star Schema DesignA star schema comprises one or more centric fact tables and each fact table links its relevant dimension tables. The star metaphor treats the fact table as the “center” of the star and the dimension table as the “point” (Moody &amp; Kortink, 2003). More specifically, the fact table corresponds to a certain business process, while the dimension tables are responsible for providing the analyzing aspects, considered as the granularity. The dimensional modeling consists of four steps, including the finding the business process; declare the granularity; defining the dimension tables and defining the fact tables. Figure 1 shows the star schema of the Fantastic Firework data warehouse. The design of this star schema will be discussed and justified with these four design steps. Figure 1 The Star Schema of Fantastic Firework Data Warehouse 3.2.1 Business ProcessesAccording to the business process of the Fantastic Fireworks company, there are two main business subjects in its business domain, including the sales and inventory. Besides, the data sources of the data warehouse are stored in the sales system and inventory system respectively, which should take into consideration when unifying the data format in the conformed dimensions. 3.2.2 GranularityGranularity addresses the lowest level of detailed information stored in the data warehouse. It is used to slice the information cube to satisfy the different information demands of end users. Consequently, different fact tables may have different granularities. Regarding the Fantastic Firework project, the sales and inventory are two main business processes(fact tables) of the data warehouse. Referring to the business requirements, the sales answers the question about which consumer has purchased how many products in which store and was served by which employee within a given time. Considering the inventory business event, it answers the question about which product has been ordered how many at a specific point of time. Therefore, the consumer, product, store, employee, and date are the identified granularity of the sales process. The product and date is the required granularity of the inventory process. 3.2.3 Dimension TablesAs we mentioned above, the dimension table is used to provide the fact tables with multiple analyzing aspects to support multiple detailed levels of information queries. Consequently, we can extract dimension tables from the granularities required by the related business processes. Figure 2 demonstrates the dimensions of each business processes and the common dimension between these two processes. Figure 2 Bus Matrix of Fantastic Firework Data Warehouse It is clear that the sales has five analyzing dimensions including consumer, product, store, employee as well as date, while the inventory just comprises product and date dimensions. It is worth mentioning that these two business processes have two common product dimensions. When designing the data structure of each dimension table, it is important to define the appropriate metadata to integrate the source data and only load the data that is needed. The following paragraphs discuss the detailed design and consideration of each dimension tables with referring to Figure 1. The indexes that are used to support fast query in each dimensional table will be discussed in detail in Appendix 2. Consumer Dimension Table Attributes: consumerID, validUnitlDate, name, dob, age, postcode Index: primary(consumerID, validUntilDate), columnstore(age) The consumer might change the address(postcode), so it is a slowly changing dimension. The validUntilDate attribute in the composite primary key can be used to distinguish customers’ valid postcode in different periods and store all the history records. Besides, the current four stores have the four independent consumer tables, which can be integrated into a single customer table in the data warehouse. Apart from the existed attributed in the original system, the age field is added to the consumer dimension table as the summarized information of the customer that can assist in grouping customer by age. Product Dimension Table Attributes: partNumber, validFromDate, description, unitPrice Index: primary(partNumber, validFromDate) The product dimension is the conformed dimension between the sales process and inventory process. It integrates the product table in the inventory system together with the product price list table in the sales system. Considering the unit price of the product might vary sometimes, the validFromDate field is used to distinguish the current price with the historical prices. Besides, the natural key including the attributes of partNumber and the validFromDate act as the composite primary key in the product dimension table. Store Dimension Table Attributes: storeID, description, address Index: primary(storeID) In the store dimension table, we directly use the natural key of the original database as the primary key and each row in the table identifies a specific store. Employee Dimension Table Attributes: employeeID, name, commissionRate Index: primary(employeeID) We directly use the natural key in our employee dimension table to distinguish employees. Each row in this table stands for a sales person. Date Dimension Table Attributes: dateID, Day, Week, Month, Quarter, Year Index: primary(dateID), column(Day/Week/Month/Quarter/Year), clustered(Day, Week, Month, Quarter, Year) The date dimension table is the basic table in almost all data warehouses. The dateID is the surrogate key in the data warehouse. We use the rest attributes to identify a time point to customizable analyzing facts of sales and inventory processes. 3.2.4 Fact TablesAs we mentioned above, the fact table corresponds to the relevant business processes. Therefore, this data warehouse consists of two fact tables including the sales fact table and the inventory fact table. Each fact table comprises the business measurements which used to provide the valuable summarized information that can support decision making. The fact table also contains the foreign keys that link with the dimension tables to do information slicing and selection. Sales Fact Table Attributes: saleID, Employee_employeeID, Consumer_consumerID, Consumer_validUntilDate, Product_partNumber, Product_validFromDate, Date_dataID, Store_storeID, unitSales, dollarSales, margin, commission Index: primary(saleID), columnstore(margin, commission, unitSales), foreign key indexes The sales fact table integrates the sales table and sales item table in the original source database. The saleID is used as the surrogate key of sales fact table rather than the oderID and line number in the original system. The sales fact table contains the additive measurements of unitSales, dollarSales, margin and commission which can be accumulated among all linked dimensions. In addition, there are five foreign keys that link to the five dimension tables respectively to do information filter and selection around the specific customer, employee, store, product and purchase time in sale records. Inventory Fact Table Attributes: inventoryID, Date_dateID, Product_partNumber, Product_validFromDate, productDescription, quantity, unitCost Index: primary(inventoryID), foreign key indexes The inventory fact table uses the inventoryID as the surrogate key to identify the inventory record uniquely. It has two foreign keys to connect with the product dimension table as well as date dimension table so as to assist the analysis of the inventory orders with a given product at a given time. 4 Key DecisionsTo begin with, there are some key attributes in the following sections that may cause misunderstanding. In order to provide a detailed explanation, some Data Dictionary Tables are given in Appendix 1. 4.1 Key CustomersIn order to identify the key customers and their age groups in a given time period, the data warehouse should provide information about the margin, unitSales, dollarSales, dateID (date dimension), customerID (customer dimension), age and postcode. In this case, the data that need to be concerned most are margin, unitSales, dollarSales measurements (in Sales Table) and all attributes in the Customer table. Some other corresponding attributes are in date dimension. Additionally, the summarized information including the dollarSales and margin of each sales record are calculated at the data load stage. The following steps demonstrate how to locate the key customers. Select all the sales records and the required information by joint the sales, customer and date table, in the given time period (e.g. All records in last year) Group the selected records which share the same consumerID Calculate the total margin, unitSales, dollarSales in each customer group Sort these group with one of the three measurements (margin, unitSales, dollarSallers) in descending order with other required information The top x sort results in the rank list can be used as the top x profitable customers. These top x customers can be grouped with the different age grades With the results of key customers, it would be possible for the Fantastic Fireworks to make business decisions. The age attribute in Customer table can be used to analyze the age distribution of the top profitable consumer. With the margin ranking, Fantastic Firework can find out the customers who bring the most profit. Also, to choose the city of new stores, some centralized user clusters must be determined. With the help of the postcode (in Customer table) and margin (in Sales table), it is easy to identify the potential cities that has the high-margin customers. These cities would be more suitable for new stores. 4.2 Most Profitable ProductThose most profitable products in a time period can be elicited by comparing the accumulated margin, unitSales, dollarSales of different products in the given time granularity (monthly, quarterly, yearly). In this case, partNumber (product dimension), dateID (date dimension), margin, unitSales and dollarSales attributes (in Sales table) are most concerned. The same calculating method as before, the margin attribute is derived by subtracting Cost (unitCost*unitSales) from dollarSales. This calculation is accomplished at the data load stage. Select all the sales records within the given time period Group the selected sales records that have the same productNumber Calculate the total margin, unitSales, dollarSales in each product group Sort these groups with one of the three measurement (margin, unitSales, dollarSales) in descending order together with the rest measurements, the productID and given time period The most profitable product is at the top of the rank lit in the given time period With the sorting list of profitable products, Fantastic Fireworks could make business decisions, like putting those highly profitable products in prominent places in the store in the particular month of the year. 4.3 Most Profitable StoreBased on the storeID (store dimension), dateID (date dimension), unitSales, dollarSales and margin attributes (in Sales table), the most profitable store in the given time period could be easily derived. Select all the sales records between the given time period (monthly in the previous year) Group the selected sales records which share the same storeID Calculate the total margin, total unitSales, total dollarSales in each group Sort the group with calculated margin (or unitSales or dollarSales) in descending order together with the rest summarized measurements, the given time period and storeID The most profitable store in each month of the previous year is on the front of the sorting list Some business decisions could be made with the help of these results. it would be wise to expand the floor space of those stores that own more sale records, earn more profits or have more dollar sales. 4.4 Most Profitable Time PeriodThe most profitable time period can be measured by adding the margin of all sales records with which the date associated with the dateID is located within the given time period. To identify the most profitable time period, the dateID (date dimension) and the margin in the sales table should be concerned. Grouping the sales records with a given time granularity(weekday, monthly, quarterly or yearly) Calculate the total margin in each group Sorting the calculated margin and the corresponding time granularity in descending order, and the most profitable time should list in the front of the result. With the help of busy time period like seasons, weekdays and monthly, Fantastic Fireworks is able to optimize store opening time more reasonable. Also, according to the current stocking strategy, most of the reordering decisions are made automatically. In order to prevent the sudden shortage of products, the manager of the inventory system could adjust the reordering decisions with the consideration of the busy time period. 4.5 Key EmployeesThe key employees could be measured by the commission or by the number of served sales records in a given time period. To identify the key employees in a specific time period, the commission (in Sales table), dateID (date dimension) and employeeID (employee dimension) should be taken into consideration. The commission in each record is calculated by dollarSales (in Sales table) multiply the corresponding commissionRate (in Employee table) according to the employeeID. Select all the sales records from sales fact table within the given time period Group the selected sales records which share the same employeeID Calculate the total commission in each group as the total commission of that employee in the given time period Sort the calculated commission with the corresponding employee information with descend order. The key employees would be seen in the front rank. The approach to get the key employee who owns the most sales records can be elicited similarly by count the number of records in each group. With these two different measurements, Fantastic Fireworks is able to evaluate its employees in different ways. Further strategies of staff management would be adjusted based on that. Appendix 1: Data Dictionary Product Table Figure 3 Product Table Inventory Table Figure 4 Inventory Table Sales Table Figure 5 Sales Table Customer Table Figure 6 Customer Table Employee Table Figure 7 Employee Table Store Table Figure 8 Store Table Date Table Figure 9 Date Table Appendix 2: SQL Statements123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0;SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0;SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=&apos;ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION&apos;;-- ------------------------------------------------------- Schema Fireworks-- ------------------------------------------------------- ------------------------------------------------------- Schema Fireworks-- -----------------------------------------------------CREATE SCHEMA IF NOT EXISTS `Fireworks` DEFAULT CHARACTER SET utf8 ;USE `Fireworks` ;-- ------------------------------------------------------- Table `Fireworks`.`Store`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Store` ( `storeID` INT NOT NULL, `description` VARCHAR(45) NULL, `address` VARCHAR(45) NULL, PRIMARY KEY (`storeID`))ENGINE = InnoDB;-- ------------------------------------------------------- Table `Fireworks`.`Product`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Product` ( `partNumber` VARCHAR(45) NOT NULL, `validFromDate` DATETIME NOT NULL, `description` VARCHAR(45) NULL, `unitPrice` DOUBLE NULL, PRIMARY KEY (`partNumber`, `validFromDate`))ENGINE = InnoDB;-- ------------------------------------------------------- Table `Fireworks`.`Customer`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Customer` ( `customerID` VARCHAR(45) NOT NULL, `validUntilDate` DATETIME NOT NULL, `name` VARCHAR(45) NULL, `dob` DATETIME NULL, `age` INT NULL, `postcode` VARCHAR(45) NULL, PRIMARY KEY (`customerID`, `validUntilDate`), INDEX `age` (`age` ASC) VISIBLE)ENGINE = InnoDB;-- ------------------------------------------------------- Table `Fireworks`.`Employee`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Employee` ( `employeeID` VARCHAR(45) NOT NULL, `name` VARCHAR(45) NULL, `commissionRate` DOUBLE NULL, PRIMARY KEY (`employeeID`))ENGINE = InnoDB;-- ------------------------------------------------------- Table `Fireworks`.`Date`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Date` ( `dateID` INT NOT NULL, `Day` INT NULL, `Week` INT NULL, `Month` INT NULL, `Quarter` INT NULL, `Year` INT NULL, PRIMARY KEY (`dateID`), INDEX `Day` (`Day` ASC) VISIBLE, INDEX `Week` (`Week` ASC) VISIBLE, INDEX `Month` (`Month` ASC) VISIBLE, INDEX `Quarter` (`Quarter` ASC) VISIBLE, INDEX `Year` (`Year` ASC) VISIBLE, INDEX `Date` (`Day` ASC, `Week` ASC, `Month` ASC, `Quarter` ASC, `Year` ASC) VISIBLE)ENGINE = InnoDB;-- ------------------------------------------------------- Table `Fireworks`.`Sales`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Sales` ( `saleID` INT NOT NULL, `Employee_employeeID` VARCHAR(45) NOT NULL, `Date_dateID` INT NOT NULL, `Store_storeID` INT NOT NULL, `Product_partNumber` VARCHAR(45) NOT NULL, `Product_validFromDate` DATETIME NOT NULL, `Customer_customerID` VARCHAR(45) NOT NULL, `Customer_validUntilDate` DATETIME NOT NULL, `unitSales` INT NULL, `salePrice` DOUBLE NULL, `dollarSales` DOUBLE NULL, `margin` DOUBLE NULL, `commission` DOUBLE NULL, PRIMARY KEY (`saleID`), INDEX `fk_Sales_Employee_idx` (`Employee_employeeID` ASC) VISIBLE, INDEX `fk_Sales_Date1_idx` (`Date_dateID` ASC) VISIBLE, INDEX `fk_SalesItem_Store1_idx` (`Store_storeID` ASC) VISIBLE, INDEX `margin` (`margin` ASC) VISIBLE, INDEX `commission` (`commission` ASC) VISIBLE, INDEX `fk_Sales_Product1_idx` (`Product_partNumber` ASC, `Product_validFromDate` ASC) VISIBLE, INDEX `fk_Sales_Customer1_idx` (`Customer_customerID` ASC, `Customer_validUntilDate` ASC) VISIBLE, INDEX `unitSales` (`unitSales` ASC) VISIBLE, CONSTRAINT `fk_Sales_Employee` FOREIGN KEY (`Employee_employeeID`) REFERENCES `Fireworks`.`Employee` (`employeeID`) ON DELETE NO ACTION ON UPDATE NO ACTION, CONSTRAINT `fk_Sales_Date1` FOREIGN KEY (`Date_dateID`) REFERENCES `Fireworks`.`Date` (`dateID`) ON DELETE NO ACTION ON UPDATE NO ACTION, CONSTRAINT `fk_SalesItem_Store1` FOREIGN KEY (`Store_storeID`) REFERENCES `Fireworks`.`Store` (`storeID`) ON DELETE NO ACTION ON UPDATE NO ACTION, CONSTRAINT `fk_Sales_Product1` FOREIGN KEY (`Product_partNumber` , `Product_validFromDate`) REFERENCES `Fireworks`.`Product` (`partNumber` , `validFromDate`) ON DELETE NO ACTION ON UPDATE NO ACTION, CONSTRAINT `fk_Sales_Customer1` FOREIGN KEY (`Customer_customerID` , `Customer_validUntilDate`) REFERENCES `Fireworks`.`Customer` (`customerID` , `validUntilDate`) ON DELETE NO ACTION ON UPDATE NO ACTION)ENGINE = InnoDB;-- ------------------------------------------------------- Table `Fireworks`.`Inventory`-- -----------------------------------------------------CREATE TABLE IF NOT EXISTS `Fireworks`.`Inventory` ( `inventoryID` INT NOT NULL, `Date_dateID` INT NOT NULL, `Product_partNumber` VARCHAR(45) NOT NULL, `Product_validFromDate` DATETIME NOT NULL, `productDescription` VARCHAR(45) NULL, `quantity` INT NULL, `unitCost` DOUBLE NULL, PRIMARY KEY (`inventoryID`), INDEX `fk_Inventory_Date1_idx` (`Date_dateID` ASC) VISIBLE, INDEX `fk_Inventory_Product1_idx` (`Product_partNumber` ASC, `Product_validFromDate` ASC) VISIBLE, CONSTRAINT `fk_Inventory_Date1` FOREIGN KEY (`Date_dateID`) REFERENCES `Fireworks`.`Date` (`dateID`) ON DELETE NO ACTION ON UPDATE NO ACTION, CONSTRAINT `fk_Inventory_Product1` FOREIGN KEY (`Product_partNumber` , `Product_validFromDate`) REFERENCES `Fireworks`.`Product` (`partNumber` , `validFromDate`) ON DELETE NO ACTION ON UPDATE NO ACTION)ENGINE = InnoDB;SET SQL_MODE=@OLD_SQL_MODE;SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS;SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS; References[1] Chaudhuri, S., &amp; Dayal, U. (1997). An overview of data warehousing and OLAP technology. ACM Sigmod record, 26(1), 65-74. [2] Kimball, R., &amp; Ross, M. (2011). The data warehouse toolkit: the complete guide to dimensional modeling. John Wiley &amp; Sons. [3] Moody, D. L., &amp; Kortink, M. A. R. (2003). From ER models to dimensional models: bridging the gap between OLTP and OLAP design, Part I. Business Intelligence Journal, 8, 7-24.","categories":[{"name":"数据","slug":"数据","permalink":"http://yoursite.com/categories/数据/"},{"name":"项目","slug":"数据/项目","permalink":"http://yoursite.com/categories/数据/项目/"},{"name":"Fantastic Fireworks","slug":"数据/项目/Fantastic-Fireworks","permalink":"http://yoursite.com/categories/数据/项目/Fantastic-Fireworks/"}],"tags":[{"name":"数据仓储","slug":"数据仓储","permalink":"http://yoursite.com/tags/数据仓储/"},{"name":"数据","slug":"数据","permalink":"http://yoursite.com/tags/数据/"}],"keywords":[{"name":"数据","slug":"数据","permalink":"http://yoursite.com/categories/数据/"},{"name":"项目","slug":"数据/项目","permalink":"http://yoursite.com/categories/数据/项目/"},{"name":"Fantastic Fireworks","slug":"数据/项目/Fantastic-Fireworks","permalink":"http://yoursite.com/categories/数据/项目/Fantastic-Fireworks/"}]}]}